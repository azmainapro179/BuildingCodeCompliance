{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GpNIa_LLRT",
        "outputId": "a619ef18-ada2-47b4-d69f-283d0a8d16f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.68.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.3)\n",
            "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading docling_core-2.59.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2026.1.4)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.4.1)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (6.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.12.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.9.0+cpu)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.26.0)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.3)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.0+cpu)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: opencv_python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading docling-2.68.0-py3-none-any.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.9/282.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.59.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.3/223.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.5.0-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (635 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=a38a293a098efed0823f016dfaeda5cb7e87dfc3722c882254ff289cfcd6d55a\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, filetype, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-c, tree-sitter, python-docx, pypdfium2, pyclipper, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.21.1\n",
            "    Uninstalling typer-0.21.1:\n",
            "      Successfully uninstalled typer-0.21.1\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.68.0 docling-core-2.59.0 docling-ibm-models-3.10.3 docling-parse-4.7.3 faker-40.1.2 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.2 mpire-2.10.2 polyfactory-3.2.0 pyclipper-1.4.0 pylatexenc-2.10 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.5.0 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnpkfsE8LR6a",
        "outputId": "198726fc-96d2-4668-be08-4d88903a6bfa"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/bnbc6.pdf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling_core/utils/file.py\u001b[0m in \u001b[0;36mresolve_source_to_stream\u001b[0;34m(source, headers)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mhttp_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAnyHttpUrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTypeAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnyHttpUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/type_adapter.py\u001b[0m in \u001b[0;36mvalidate_python\u001b[0;34m(self, object, strict, extra, from_attributes, context, experimental_allow_partial, by_alias, by_name)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         return self.validator.validate_python(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for function-wrap[wrap_val()]\n  Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='/content/bnbc6.pdf', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/url_parsing",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2411729379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Full Docling JSON export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_validate_call.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# We need to manually update this because `partial` object has no `__name__` and `__qualname__`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_validate_call.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_validators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydantic_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgsKwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__return_pydantic_validator__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__return_pydantic_validator__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling/document_converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mpage_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         )\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mvalidate_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfigDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling/document_converter.py\u001b[0m in \u001b[0;36mconvert_all\u001b[0;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mhad_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mconv_res\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_res_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mhad_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             if raises_on_error and conv_res.status not in {\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling/document_converter.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(self, conv_input, raises_on_error)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         for input_batch in chunkify(\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mconv_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_to_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_batch_size\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pass format_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling/utils/utils.py\u001b[0m in \u001b[0;36mchunkify\u001b[0;34m(iterator, chunk_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Take the first element from the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling/datamodel/document.py\u001b[0m in \u001b[0;36mdocs\u001b[0;34m(self, format_options)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_stream_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             obj = (\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0mresolve_source_to_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/docling_core/utils/file.py\u001b[0m in \u001b[0;36mresolve_source_to_stream\u001b[0;34m(source, headers)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTypeAdapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mdoc_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mread_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mOpen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \"\"\"\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/bnbc6.pdf'"
          ]
        }
      ],
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "\n",
        "source = \"/content/bnbc6.pdf\"\n",
        "\n",
        "# Enable formula enrichment so equations become TextItems with label FORMULA and carry LaTeX text\n",
        "pipeline_options = PdfPipelineOptions()\n",
        "pipeline_options.do_formula_enrichment = True\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "    }\n",
        ")\n",
        "\n",
        "result = converter.convert(source)\n",
        "\n",
        "# Full Docling JSON export\n",
        "structured_json = result.document.export_to_dict()\n",
        "\n",
        "print(\"Converted. First text item:\", structured_json[\"texts\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5qhHbR6LyYb",
        "outputId": "56f8f255-c0df-4845-cdaf-a94e5d7ba4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success! Your structured data is saved in bnbc6.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "output_filename = \"bnbc6.json\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Success! Your structured data is saved in {output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWdqL4VtcgbB",
        "outputId": "b8af44ae-3453-4948-90af-b6b28c4d0ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.7\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSaeLuiU9VM",
        "outputId": "d92efcb2-8871-47d1-96a8-faffcba409f4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'bnbc6.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1702208708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1702208708.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0meq_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_base\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEQUATIONS_DIR_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_docling_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DOCLING_JSON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;31m# 1) Clause tree (with figure captions + equation latex refs + nested points)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1702208708.py\u001b[0m in \u001b[0;36mload_docling_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_docling_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bnbc6.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# ----------------------------\n",
        "# Config (edit these paths)\n",
        "# ----------------------------\n",
        "INPUT_DOCLING_JSON = \"/content/bnbc6.json\"\n",
        "INPUT_PDF = \"/content/bnbc6.pdf\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/structured_out\"\n",
        "IMAGES_DIR_NAME = \"images\"\n",
        "EQUATIONS_DIR_NAME = \"equations\"\n",
        "DPI = 200\n",
        "IMAGE_FORMAT = \"jpg\"\n",
        "\n",
        "# If Docling formula enrichment produces spaced-out LaTeX (each char separated by spaces),\n",
        "# enable this heuristic de-spacing fix.\n",
        "FIX_SPACED_LATEX = True\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: Docling JSON access\n",
        "# ----------------------------\n",
        "CLAUSE_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)\\s+(.*\\S)\\s*$\")\n",
        "\n",
        "# List marker patterns (start-of-line)\n",
        "PAREN_MARKER_RE = re.compile(r\"^\\s*\\(\\s*([A-Za-z0-9]+|[ivxlcdmIVXLCDM]+)\\s*\\)\\s+\")\n",
        "SIMPLE_MARKER_RE = re.compile(r\"^\\s*([A-Za-z]|\\d+|[ivxlcdmIVXLCDM]+)([\\.|\\)])\\s+\")\n",
        "BULLET_MARKER_RE = re.compile(r\"^\\s*([•\\-*–—])\\s+\")\n",
        "\n",
        "# Inline markers inside a single paragraph, e.g. \"... requirements: (1) ... (2) ... (3) ...\"\n",
        "# We only treat a parenthesized token as a marker if it's preceded by start/whitespace/:/; and followed by space.\n",
        "INLINE_PAREN_MARKER_RE = re.compile(r\"(?:(?<=^)|(?<=[\\s:;]))\\(\\s*([0-9]+|[A-Za-z]|[ivxlcdmIVXLCDM]+)\\s*\\)\\s+\")\n",
        "\n",
        "\n",
        "def normalize_ws(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
        "\n",
        "\n",
        "def load_docling_json(path: str) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def resolve_ref(doc: dict, ref: str):\n",
        "    \"\"\"\n",
        "    ref examples:\n",
        "      '#/texts/93'\n",
        "      '#/tables/0'\n",
        "      '#/pictures/2'\n",
        "      '#/groups/5'\n",
        "    \"\"\"\n",
        "    m = re.match(r\"^#/(texts|tables|pictures|groups)/(\\d+)$\", ref)\n",
        "    if not m:\n",
        "        return None, None, None\n",
        "    kind, idx = m.group(1), int(m.group(2))\n",
        "    return kind, idx, doc[kind][idx]\n",
        "\n",
        "\n",
        "def extract_caption(doc: dict, captions) -> str:\n",
        "    \"\"\"Extract concatenated caption text from refs like [{'$ref':'#/texts/93'}].\"\"\"\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for c in captions:\n",
        "        r = c.get(\"$ref\")\n",
        "        if not r:\n",
        "            continue\n",
        "        kind, _, obj = resolve_ref(doc, r)\n",
        "        if kind == \"texts\":\n",
        "            t = normalize_ws(obj.get(\"text\") or \"\")\n",
        "            if t:\n",
        "                parts.append(t)\n",
        "    return \" \".join(parts).strip()\n",
        "\n",
        "\n",
        "def walk_body_in_reading_order(doc: dict):\n",
        "    \"\"\"\n",
        "    Body children contains a mixture of texts/tables/pictures and also '#/groups/*'.\n",
        "    We DFS into groups so we don't miss content.\n",
        "    \"\"\"\n",
        "    body = doc.get(\"body\") or {}\n",
        "    stack = []\n",
        "\n",
        "    def push_children(children):\n",
        "        for child in reversed(children or []):\n",
        "            stack.append(child)\n",
        "\n",
        "    push_children(body.get(\"children\"))\n",
        "\n",
        "    while stack:\n",
        "        item = stack.pop()\n",
        "        if not isinstance(item, dict) or \"$ref\" not in item:\n",
        "            continue\n",
        "\n",
        "        ref = item[\"$ref\"]\n",
        "        kind, _, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"groups\":\n",
        "            push_children(obj.get(\"children\"))\n",
        "            continue\n",
        "\n",
        "        yield ref\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Generic point parsing utilities\n",
        "# ----------------------------\n",
        "def leading_indent_spaces(s: str) -> int:\n",
        "    if not s:\n",
        "        return 0\n",
        "    n = 0\n",
        "    for ch in s:\n",
        "        if ch == \" \":\n",
        "            n += 1\n",
        "        elif ch == \"\\t\":\n",
        "            n += 4\n",
        "        else:\n",
        "            break\n",
        "    return n\n",
        "\n",
        "\n",
        "def parse_list_marker(raw_text: str, marker_field: str = \"\", allow_inline_start: bool = True):\n",
        "    \"\"\"\n",
        "    Returns (marker, content, indent_spaces) if this looks like a point item; else (None, None, None).\n",
        "\n",
        "    Priority:\n",
        "      1) Docling-provided marker_field (e.g., '1.' or '*')\n",
        "      2) '(a) ...', '(1) ...', '(i) ...' at START of text\n",
        "      3) 'a) ...', '1. ...', 'i) ...' at START of text\n",
        "      4) bullet char at START of text\n",
        "    \"\"\"\n",
        "    if raw_text is None:\n",
        "        return None, None, None\n",
        "\n",
        "    indent = leading_indent_spaces(raw_text)\n",
        "    s = raw_text.lstrip(\"\\t \").rstrip()\n",
        "\n",
        "    mf = (marker_field or \"\").strip()\n",
        "    if mf:\n",
        "        return mf, normalize_ws(s), indent\n",
        "\n",
        "    if not allow_inline_start or not s:\n",
        "        return None, None, None\n",
        "\n",
        "    m = PAREN_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"({m.group(1)})\"\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    m = SIMPLE_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"{m.group(1)}{m.group(2)}\"\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    m = BULLET_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = m.group(1)\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def split_inline_enumeration(raw_text: str):\n",
        "    \"\"\"\n",
        "    If raw_text contains multiple inline markers like:\n",
        "      \"Intro: (1) aaa (2) bbb (3) ccc\"\n",
        "    return:\n",
        "      intro=\"Intro:\" and items=[{\"marker\":\"(1)\",\"text\":\"aaa\"}, ...]\n",
        "    Otherwise returns (None, None).\n",
        "    \"\"\"\n",
        "    if not raw_text:\n",
        "        return None, None\n",
        "    s = raw_text.strip()\n",
        "    matches = list(INLINE_PAREN_MARKER_RE.finditer(s))\n",
        "    if len(matches) < 2:\n",
        "        return None, None\n",
        "\n",
        "    intro = s[:matches[0].start()].strip()\n",
        "    items = []\n",
        "    for i, mm in enumerate(matches):\n",
        "        mk = f\"({mm.group(1)})\"\n",
        "        start = mm.end()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(s)\n",
        "        content = normalize_ws(s[start:end]).strip(\" ;\")\n",
        "        if content:\n",
        "            items.append({\"marker\": mk, \"text\": content})\n",
        "    if not items:\n",
        "        return None, None\n",
        "    return intro, items\n",
        "\n",
        "\n",
        "def is_noise_text(s: str) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic to ignore headers/footers/page numbers that can appear between list items.\n",
        "    This is intentionally generic (no document-specific keywords).\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return True\n",
        "    t = s.strip()\n",
        "    if not t:\n",
        "        return True\n",
        "    if re.fullmatch(r\"\\d{3,6}\", t):  # page numbers like 3235\n",
        "        return True\n",
        "    if len(t) <= 12 and sum(ch.isalpha() for ch in t) < 2:\n",
        "        return True\n",
        "    # very punctuation-heavy short lines\n",
        "    if len(t) <= 25 and sum(ch.isalnum() for ch in t) / max(1, len(t)) < 0.35:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Reading-order sort key (for list runs)\n",
        "# ----------------------------\n",
        "def _reading_order_key(block: dict):\n",
        "    \"\"\"Sort by page then top-to-bottom then left-to-right using bbox.\"\"\"\n",
        "    page_no = block.get(\"page_no\")\n",
        "    bbox = block.get(\"bbox\") or {}\n",
        "    l = bbox.get(\"l\") if isinstance(bbox, dict) else None\n",
        "    t = bbox.get(\"t\") if isinstance(bbox, dict) else None\n",
        "    origin = (bbox.get(\"coord_origin\") or \"TOPLEFT\").upper() if isinstance(bbox, dict) else \"TOPLEFT\"\n",
        "\n",
        "    page_key = page_no if page_no is not None else 10**9\n",
        "    y_key = 0 if t is None else ((-t) if origin == \"BOTTOMLEFT\" else t)\n",
        "    x_key = l if l is not None else 0\n",
        "    return (page_key, y_key, x_key)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Smart nesting (indent + marker kind + lookahead)\n",
        "# ----------------------------\n",
        "def _is_roman(s: str) -> bool:\n",
        "    if not s:\n",
        "        return False\n",
        "    s = s.strip().lower()\n",
        "    return bool(re.fullmatch(r\"[ivxlcdm]+\", s))\n",
        "\n",
        "\n",
        "def marker_kind(marker: str) -> str:\n",
        "    if not marker:\n",
        "        return \"other\"\n",
        "    m = marker.strip()\n",
        "\n",
        "    if m in {\"•\", \"-\", \"*\", \"–\", \"—\"}:\n",
        "        return \"bullet\"\n",
        "\n",
        "    core = re.sub(r\"^[\\(\\[]\\s*\", \"\", m)\n",
        "    core = re.sub(r\"\\s*[\\)\\]]$\", \"\", core)\n",
        "    core = re.sub(r\"[\\.)]$\", \"\", core).strip()\n",
        "\n",
        "    if core.isdigit():\n",
        "        return \"num\"\n",
        "    if len(core) == 1 and core.isalpha():\n",
        "        return \"alpha\"\n",
        "    if _is_roman(core):\n",
        "        return \"roman\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "KIND_ORDER = {\"num\": 0, \"alpha\": 1, \"roman\": 2, \"bullet\": 3, \"other\": 3}\n",
        "\n",
        "\n",
        "def _nesting_trigger_text(s: str) -> bool:\n",
        "    \"\"\"Non-hardcoded cue: a colon usually introduces a sublist.\"\"\"\n",
        "    return bool(s and s.strip().endswith(\":\"))\n",
        "\n",
        "\n",
        "def _will_return_to_kind(flat_items, start_idx: int, target_kind: str) -> bool:\n",
        "    \"\"\"Lookahead: if we later return to target_kind, items in between are likely a sublist.\"\"\"\n",
        "    for j in range(start_idx + 1, len(flat_items)):\n",
        "        k = marker_kind(flat_items[j].get(\"marker\", \"\"))\n",
        "        if k == target_kind:\n",
        "            return True\n",
        "        if KIND_ORDER.get(k, 3) < KIND_ORDER.get(target_kind, 3):\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "\n",
        "def nest_list_items_smart(flat_items):\n",
        "    \"\"\"\n",
        "    Build a nested list tree using:\n",
        "      1) indentation (from bbox.l when available)\n",
        "      2) marker-kind hierarchy + ':' cue + lookahead return\n",
        "    \"\"\"\n",
        "    root = []\n",
        "    stack = []  # entries: {\"indent\": float, \"kind\": str, \"order\": int, \"node\": dict}\n",
        "\n",
        "    for i, it in enumerate(flat_items):\n",
        "        node = {\"marker\": it.get(\"marker\", \"\"), \"text\": it.get(\"text\", \"\"), \"children\": []}\n",
        "        ind = float(it.get(\"indent\") or 0)\n",
        "\n",
        "        k = marker_kind(node[\"marker\"])\n",
        "        k_order = KIND_ORDER.get(k, 3)\n",
        "\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        prev = stack[-1]\n",
        "\n",
        "        # Indent-based nesting when it changes meaningfully.\n",
        "        indent_diff = ind - prev[\"indent\"]\n",
        "        indent_is_informative = abs(indent_diff) >= 5.0  # bbox units are typically points\n",
        "\n",
        "        if indent_is_informative:\n",
        "            while stack and ind <= stack[-1][\"indent\"]:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # Marker-based inference when indent isn't helpful.\n",
        "        prev_kind = prev[\"kind\"]\n",
        "        prev_order = prev[\"order\"]\n",
        "\n",
        "        prev_introduces_sublist = _nesting_trigger_text(prev[\"node\"].get(\"text\", \"\"))\n",
        "        lookahead_sublist = _will_return_to_kind(flat_items, i, prev_kind)\n",
        "\n",
        "        if (k_order > prev_order) and (prev_introduces_sublist or lookahead_sublist):\n",
        "            prev[\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        if k == prev_kind:\n",
        "            # sibling\n",
        "            stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        if k_order <= prev_order:\n",
        "            while stack and KIND_ORDER.get(stack[-1][\"kind\"], 3) >= k_order:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # default: sibling\n",
        "        stack.pop()\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "        else:\n",
        "            stack[-1][\"node\"][\"children\"].append(node)\n",
        "        stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "def blocks_to_text_and_lists(blocks):\n",
        "    \"\"\"\n",
        "    blocks: list of either\n",
        "      {\"kind\":\"text\",\"text\":...}\n",
        "      {\"kind\":\"list_item\",\"marker\":..., \"text\":..., \"indent\":..., \"page_no\":..., \"bbox\":...}\n",
        "\n",
        "    Returns:\n",
        "      text: merged paragraphs (non-list blocks)\n",
        "      lists: list of {\"items\":[nested...]} in reading order\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "    lists = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(blocks):\n",
        "        b = blocks[i]\n",
        "        if b[\"kind\"] == \"text\":\n",
        "            t = b.get(\"text\", \"\")\n",
        "            if t and not is_noise_text(t):\n",
        "                text_parts.append(t.strip())\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # collect list items, skipping noise text between them\n",
        "        j = i\n",
        "        run = []\n",
        "        while j < len(blocks):\n",
        "            bb = blocks[j]\n",
        "            if bb[\"kind\"] == \"list_item\":\n",
        "                run.append(bb)\n",
        "                j += 1\n",
        "                continue\n",
        "            if bb[\"kind\"] == \"text\" and is_noise_text(bb.get(\"text\", \"\")):\n",
        "                j += 1\n",
        "                continue\n",
        "            break\n",
        "\n",
        "        # Sort by bbox order if we have provenance (fixes e.g., 9, 11, 10).\n",
        "        if any((x.get(\"page_no\") is not None and x.get(\"bbox\")) for x in run):\n",
        "            run = sorted(run, key=_reading_order_key)\n",
        "\n",
        "        nested = nest_list_items_smart(run)\n",
        "        lists.append({\"items\": nested})\n",
        "\n",
        "        i = j\n",
        "\n",
        "    text = \"\\n\".join([p for p in text_parts if p]).strip()\n",
        "    return text, lists\n",
        "\n",
        "\n",
        "def format_list_items(items, level=0):\n",
        "    \"\"\"Render nested list items into plain text (for retrieval display).\"\"\"\n",
        "    out = []\n",
        "    indent = \"  \" * level\n",
        "    for it in items or []:\n",
        "        mk = it.get(\"marker\", \"\")\n",
        "        tx = it.get(\"text\", \"\")\n",
        "        line = f\"{indent}{mk} {tx}\".strip()\n",
        "        if line:\n",
        "            out.append(line)\n",
        "        if it.get(\"children\"):\n",
        "            out.append(format_list_items(it[\"children\"], level + 1))\n",
        "    return \"\\n\".join([x for x in out if x]).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Tables (robust)\n",
        "# ----------------------------\n",
        "def table_rows_robust(table_obj: dict):\n",
        "    \"\"\"\n",
        "    Robustly converts Docling tables into rows[][].\n",
        "\n",
        "    Handles:\n",
        "      A) data[\"grid\"][r][c] is a dict with \"text\"\n",
        "      B) data[\"grid\"][r][c] is an int index into data[\"table_cells\"]\n",
        "      C) missing/empty grid -> reconstruct from table_cells span metadata\n",
        "    \"\"\"\n",
        "    data = table_obj.get(\"data\") or {}\n",
        "    grid = data.get(\"grid\") or []\n",
        "    table_cells = data.get(\"table_cells\") or []\n",
        "\n",
        "    num_rows = int(data.get(\"num_rows\") or (len(grid) if grid else 0))\n",
        "    num_cols = int(data.get(\"num_cols\") or (len(grid[0]) if grid and grid[0] else 0))\n",
        "\n",
        "    if grid and grid[0] and isinstance(grid[0][0], dict):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            rows.append([normalize_ws(cell.get(\"text\", \"\")) if isinstance(cell, dict) else \"\" for cell in r])\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    if grid and grid[0] and isinstance(grid[0][0], int):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for idx in r:\n",
        "                if isinstance(idx, int) and 0 <= idx < len(table_cells):\n",
        "                    row.append(normalize_ws(table_cells[idx].get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    mat = [[\"\" for _ in range(num_cols)] for _ in range(num_rows)]\n",
        "    for cell in table_cells:\n",
        "        txt = normalize_ws(cell.get(\"text\", \"\"))\n",
        "        r0 = cell.get(\"start_row_offset_idx\", 0)\n",
        "        r1 = cell.get(\"end_row_offset_idx\", r0 + 1)\n",
        "        c0 = cell.get(\"start_col_offset_idx\", 0)\n",
        "        c1 = cell.get(\"end_col_offset_idx\", c0 + 1)\n",
        "        for rr in range(r0, r1):\n",
        "            for cc in range(c0, c1):\n",
        "                if 0 <= rr < num_rows and 0 <= cc < num_cols:\n",
        "                    mat[rr][cc] = txt\n",
        "\n",
        "    return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": mat}\n",
        "\n",
        "\n",
        "def build_tables_json(doc: dict) -> list:\n",
        "    out = []\n",
        "    for i, tbl in enumerate(doc.get(\"tables\", [])):\n",
        "        provs = tbl.get(\"prov\") or []\n",
        "        page_no = provs[0].get(\"page_no\") if provs else None\n",
        "        bbox = provs[0].get(\"bbox\") if provs else None\n",
        "        caption = extract_caption(doc, tbl.get(\"captions\"))\n",
        "        t = table_rows_robust(tbl)\n",
        "        out.append(\n",
        "            {\n",
        "                \"table_id\": f\"table_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"num_rows\": t[\"num_rows\"],\n",
        "                \"num_cols\": t[\"num_cols\"],\n",
        "                \"rows\": t[\"rows\"],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Images & geometry helpers\n",
        "# ----------------------------\n",
        "def bbox_to_fitz_rect(bbox: dict, page_height: float) -> fitz.Rect | None:\n",
        "    \"\"\"\n",
        "    Docling bbox has coord_origin, often 'BOTTOMLEFT'.\n",
        "    PyMuPDF uses TOPLEFT origin.\n",
        "    \"\"\"\n",
        "    l = bbox.get(\"l\")\n",
        "    t = bbox.get(\"t\")\n",
        "    r = bbox.get(\"r\")\n",
        "    b = bbox.get(\"b\")\n",
        "    if None in (l, t, r, b):\n",
        "        return None\n",
        "\n",
        "    origin = (bbox.get(\"coord_origin\") or \"TOPLEFT\").upper()\n",
        "\n",
        "    if origin == \"BOTTOMLEFT\":\n",
        "        y0 = page_height - t\n",
        "        y1 = page_height - b\n",
        "    else:\n",
        "        y0 = t\n",
        "        y1 = b\n",
        "\n",
        "    x0, x1 = sorted([l, r])\n",
        "    y0, y1 = sorted([y0, y1])\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "\n",
        "def extract_figures_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    images_meta = []\n",
        "\n",
        "    for i, pic in enumerate(doc.get(\"pictures\", [])):\n",
        "        provs = pic.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page.rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page.rect\n",
        "        caption = extract_caption(doc, pic.get(\"captions\"))\n",
        "\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        fname = f\"figure_{i:04d}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        images_meta.append(\n",
        "            {\n",
        "                \"figure_id\": f\"figure_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return images_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Equations (FORMULA items)\n",
        "# ----------------------------\n",
        "def is_formula_text_item(text_obj: dict) -> bool:\n",
        "    return (text_obj.get(\"label\") or \"\").strip().upper() == \"FORMULA\"\n",
        "\n",
        "\n",
        "def maybe_despace_latex(latex: str) -> str:\n",
        "    s = latex.strip()\n",
        "    toks = s.split()\n",
        "    if len(toks) < 8:\n",
        "        return s\n",
        "    single = sum(1 for t in toks if len(t) == 1)\n",
        "    if single / max(1, len(toks)) >= 0.6:\n",
        "        return \"\".join(toks)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_formula_latex(text_obj: dict) -> str:\n",
        "    raw = text_obj.get(\"latex\") or text_obj.get(\"text\") or \"\"\n",
        "    raw = normalize_ws(raw.strip())\n",
        "    return maybe_despace_latex(raw) if FIX_SPACED_LATEX else raw\n",
        "\n",
        "\n",
        "def extract_equations_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    equations_meta = []\n",
        "\n",
        "    for idx, t in enumerate(doc.get(\"texts\", []) or []):\n",
        "        if not is_formula_text_item(t):\n",
        "            continue\n",
        "\n",
        "        provs = t.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page.rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page.rect\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        eq_id = f\"eq_{idx:05d}\"\n",
        "        fname = f\"{eq_id}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        equations_meta.append(\n",
        "            {\n",
        "                \"equation_id\": eq_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"latex\": extract_formula_latex(t),\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return equations_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Clause tree\n",
        "# ----------------------------\n",
        "def build_clause_tree(doc: dict):\n",
        "    \"\"\"\n",
        "    Builds nodes keyed by clause id.\n",
        "    Stores:\n",
        "      - text: paragraphs (excluding list items)\n",
        "      - lists: nested list blocks\n",
        "      - equations: refs + latex\n",
        "      - figures: captions only\n",
        "      - tables: refs + captions\n",
        "    \"\"\"\n",
        "    nodes = {}\n",
        "    root_id = \"ROOT\"\n",
        "    nodes[root_id] = {\n",
        "        \"id\": root_id,\n",
        "        \"title\": \"\",\n",
        "        \"children\": [],\n",
        "        \"tables\": [],\n",
        "        \"figures\": [],\n",
        "        \"equations\": [],\n",
        "        \"text\": \"\",\n",
        "        \"lists\": [],\n",
        "        \"_blocks\": [],\n",
        "    }\n",
        "    current_id = root_id\n",
        "\n",
        "    def ensure_node(cid: str):\n",
        "        if cid not in nodes:\n",
        "            nodes[cid] = {\n",
        "                \"id\": cid,\n",
        "                \"title\": \"\",\n",
        "                \"children\": [],\n",
        "                \"tables\": [],\n",
        "                \"figures\": [],\n",
        "                \"equations\": [],\n",
        "                \"text\": \"\",\n",
        "                \"lists\": [],\n",
        "                \"_blocks\": [],\n",
        "            }\n",
        "\n",
        "    def parent_id(cid: str) -> str:\n",
        "        parts = cid.split(\".\")\n",
        "        return root_id if len(parts) <= 1 else \".\".join(parts[:-1])\n",
        "\n",
        "    def add_child(pid: str, cid: str):\n",
        "        if cid not in nodes[pid][\"children\"]:\n",
        "            nodes[pid][\"children\"].append(cid)\n",
        "\n",
        "    def add_text_block(cid: str, txt: str):\n",
        "        txt = (txt or \"\").rstrip()\n",
        "        if not txt:\n",
        "            return\n",
        "        nodes[cid][\"_blocks\"].append({\"kind\": \"text\", \"text\": txt})\n",
        "\n",
        "    def add_list_block(cid: str, marker: str, txt: str, indent: float, page_no=None, bbox=None):\n",
        "        nodes[cid][\"_blocks\"].append(\n",
        "            {\n",
        "                \"kind\": \"list_item\",\n",
        "                \"marker\": marker,\n",
        "                \"text\": txt,\n",
        "                \"indent\": indent,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    for ref in walk_body_in_reading_order(doc):\n",
        "        kind, idx, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"texts\":\n",
        "            label = (obj.get(\"label\") or \"\").strip().lower()\n",
        "            raw = obj.get(\"text\") or \"\"\n",
        "            if not raw.strip():\n",
        "                continue\n",
        "\n",
        "            provs = obj.get(\"prov\") or []\n",
        "            prov0 = provs[0] if provs else {}\n",
        "            page_no = prov0.get(\"page_no\")\n",
        "            bbox = prov0.get(\"bbox\")\n",
        "            # Use bbox.l as a geometry-based indent when available (more reliable than whitespace).\n",
        "            geo_indent = float(bbox.get(\"l\")) if isinstance(bbox, dict) and bbox.get(\"l\") is not None else 0.0\n",
        "\n",
        "            # Clause header detection first\n",
        "            m = CLAUSE_RE.match(raw.strip())\n",
        "            if m:\n",
        "                cid = m.group(1)\n",
        "                rest = m.group(2).strip()\n",
        "\n",
        "                ensure_node(cid)\n",
        "                pid = parent_id(cid)\n",
        "                ensure_node(pid)\n",
        "                add_child(pid, cid)\n",
        "\n",
        "                depth = len(cid.split(\".\"))\n",
        "\n",
        "                if depth <= 3:\n",
        "                    if not nodes[cid][\"title\"]:\n",
        "                        nodes[cid][\"title\"] = rest\n",
        "                else:\n",
        "                    add_text_block(cid, rest)\n",
        "\n",
        "                current_id = cid\n",
        "                continue\n",
        "\n",
        "            # Equations\n",
        "            if is_formula_text_item(obj):\n",
        "                eq_id = f\"eq_{idx:05d}\"\n",
        "                latex = extract_formula_latex(obj)\n",
        "                nodes[current_id][\"equations\"].append({\"equation_id\": eq_id, \"latex\": latex})\n",
        "                add_text_block(current_id, f\"$$ {latex} $$\" if latex else f\"[EQ {eq_id}]\")\n",
        "                continue\n",
        "\n",
        "            # Inline enumerations inside a paragraph: (1)...(2)...(3)...\n",
        "            if label != \"list_item\":\n",
        "                intro, items = split_inline_enumeration(raw)\n",
        "                if items:\n",
        "                    if intro:\n",
        "                        add_text_block(current_id, normalize_ws(intro))\n",
        "                    for it2 in items:\n",
        "                        add_list_block(current_id, it2[\"marker\"], it2[\"text\"], indent=geo_indent, page_no=page_no, bbox=bbox)\n",
        "                    continue\n",
        "\n",
        "            # Regular list items\n",
        "            marker_field = obj.get(\"marker\") or \"\"\n",
        "            mk, content, indent_spaces = parse_list_marker(raw_text=raw, marker_field=marker_field, allow_inline_start=True)\n",
        "\n",
        "            if label == \"list_item\" or mk:\n",
        "                if mk and content:\n",
        "                    add_list_block(current_id, mk, content, indent=geo_indent if geo_indent else float(indent_spaces), page_no=page_no, bbox=bbox)\n",
        "                else:\n",
        "                    add_text_block(current_id, normalize_ws(raw))\n",
        "            else:\n",
        "                add_text_block(current_id, normalize_ws(raw))\n",
        "\n",
        "        elif kind == \"tables\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"tables\"].append({\"table_id\": f\"table_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "        elif kind == \"pictures\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"figures\"].append({\"figure_id\": f\"figure_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "    # Finalize blocks into text + nested lists\n",
        "    for nid in list(nodes.keys()):\n",
        "        blocks = nodes[nid].get(\"_blocks\", [])\n",
        "        text, lists = blocks_to_text_and_lists(blocks)\n",
        "        nodes[nid][\"text\"] = text\n",
        "        nodes[nid][\"lists\"] = lists\n",
        "        nodes[nid].pop(\"_blocks\", None)\n",
        "\n",
        "    return {\"root\": root_id, \"nodes\": nodes}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Retrieval helper (optional)\n",
        "# ----------------------------\n",
        "def collect_text_recursive(structured: dict, clause_id: str) -> str:\n",
        "    nodes = structured[\"nodes\"]\n",
        "    if clause_id not in nodes:\n",
        "        return \"\"\n",
        "    n = nodes[clause_id]\n",
        "    chunks = []\n",
        "\n",
        "    if clause_id != \"ROOT\" and n.get(\"title\"):\n",
        "        chunks.append(f\"{clause_id} {n['title']}\".strip())\n",
        "    if n.get(\"text\"):\n",
        "        chunks.append(n[\"text\"])\n",
        "\n",
        "    for lst in n.get(\"lists\", []):\n",
        "        rendered = format_list_items(lst.get(\"items\", []))\n",
        "        if rendered:\n",
        "            chunks.append(rendered)\n",
        "\n",
        "    for fig in n.get(\"figures\", []):\n",
        "        if fig.get(\"caption\"):\n",
        "            chunks.append(fig[\"caption\"])\n",
        "    for tbl in n.get(\"tables\", []):\n",
        "        if tbl.get(\"caption\"):\n",
        "            chunks.append(tbl[\"caption\"])\n",
        "    for eq in n.get(\"equations\", []):\n",
        "        if eq.get(\"latex\"):\n",
        "            chunks.append(f\"$$ {eq['latex']} $$\")\n",
        "\n",
        "    for child in n.get(\"children\", []):\n",
        "        child_txt = collect_text_recursive(structured, child)\n",
        "        if child_txt:\n",
        "            chunks.append(child_txt)\n",
        "\n",
        "    return \"\\n\".join(chunks).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    out_base = Path(OUTPUT_DIR)\n",
        "    out_base.mkdir(parents=True, exist_ok=True)\n",
        "    images_dir = out_base / IMAGES_DIR_NAME\n",
        "    eq_dir = out_base / EQUATIONS_DIR_NAME\n",
        "\n",
        "    doc = load_docling_json(INPUT_DOCLING_JSON)\n",
        "\n",
        "    clauses_struct = build_clause_tree(doc)\n",
        "    tables_struct = build_tables_json(doc)\n",
        "    images_struct = extract_figures_from_pdf(doc, INPUT_PDF, str(images_dir), dpi=DPI, image_format=IMAGE_FORMAT)\n",
        "    equations_struct = extract_equations_from_pdf(doc, INPUT_PDF, str(eq_dir), dpi=DPI, image_format=IMAGE_FORMAT)\n",
        "\n",
        "    clauses_path = out_base / \"structured_clauses.json\"\n",
        "    tables_path = out_base / \"structured_tables.json\"\n",
        "    images_path = out_base / \"structured_images.json\"\n",
        "    equations_path = out_base / \"structured_equations.json\"\n",
        "\n",
        "    clauses_path.write_text(json.dumps(clauses_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    tables_path.write_text(json.dumps(tables_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    images_path.write_text(json.dumps(images_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    equations_path.write_text(json.dumps(equations_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # quick stats\n",
        "    nodes = clauses_struct[\"nodes\"]\n",
        "    list_blocks = sum(len(n.get(\"lists\", [])) for n in nodes.values())\n",
        "    list_items = 0\n",
        "    def count_items(items):\n",
        "        nonlocal list_items\n",
        "        for it in items:\n",
        "            list_items += 1\n",
        "            count_items(it.get(\"children\", []))\n",
        "    for n in nodes.values():\n",
        "        for lst in n.get(\"lists\", []):\n",
        "            count_items(lst.get(\"items\", []))\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    print(\" -\", clauses_path)\n",
        "    print(\" -\", tables_path)\n",
        "    print(\" -\", images_path)\n",
        "    print(\" -\", equations_path)\n",
        "    print(\"\\nCounts:\")\n",
        "    print(\" clauses:\", len(nodes))\n",
        "    print(\" tables:\", len(tables_struct))\n",
        "    print(\" figures:\", len(images_struct))\n",
        "    print(\" equations:\", len(equations_struct))\n",
        "    print(\" list blocks:\", list_blocks)\n",
        "    print(\" list items:\", list_items)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRQtuEAZcmzS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}