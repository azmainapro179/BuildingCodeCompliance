{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GpNIa_LLRT",
        "outputId": "6c498064-0fd9-47f3-b3ba-9ab51588df62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.67.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.3)\n",
            "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading docling_core-2.58.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2025.11.12)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.4.1)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (6.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.12.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.9.0+cpu)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.25.1)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tree-sitter-java-orchard<1.0.0,>=0.3.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_java_orchard-0.4.2.tar.gz (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.3)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.0+cpu)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-40.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: opencv_python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading docling-2.67.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.8/277.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.58.1-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.0/224.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.5.0-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.1.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (635 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc, tree-sitter-java-orchard\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=91bf7c6aa4ad29d5cb7a196f7bd70968be18ae6822b3a0343d53a20484418c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "  Building wheel for tree-sitter-java-orchard (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree-sitter-java-orchard: filename=tree_sitter_java_orchard-0.4.2-cp310-abi3-linux_x86_64.whl size=84315 sha256=e835f3a198a366d0126101fa196d929837076ea51757b29ac6a44198967c7160\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/b5/af/ddd68e2bd7e9a86b20a2a16c34327631defdece9e05c16c1a6\n",
            "Successfully built pylatexenc tree-sitter-java-orchard\n",
            "Installing collected packages: pylatexenc, filetype, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-java-orchard, tree-sitter-c, tree-sitter, python-docx, pypdfium2, pyclipper, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.20.0\n",
            "    Uninstalling typer-0.20.0:\n",
            "      Successfully uninstalled typer-0.20.0\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.67.0 docling-core-2.58.1 docling-ibm-models-3.10.3 docling-parse-4.7.2 faker-40.1.0 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.2 mpire-2.10.2 polyfactory-3.2.0 pyclipper-1.4.0 pylatexenc-2.10 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.5.0 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-java-orchard-0.4.2 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "source = \"/content/Bangladesh-National-Building-Code-chap1.pdf\"\n",
        "converter = DocumentConverter()\n",
        "result = converter.convert(source)\n",
        "\n",
        "# This gives you the full hierarchical JSON\n",
        "structured_json = result.document.export_to_dict()\n",
        "\n",
        "# Now you can programmatically find a specific section\n",
        "# (Example: Accessing the title of the first identified section)\n",
        "print(structured_json[\"texts\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnpkfsE8LR6a",
        "outputId": "8761b619-4dee-47d0-ea52-0a54cc988cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2026-01-12 12:35:04,589 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:04,595 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:04,601 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:07,267 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:08,093 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:08,095 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:08,465 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:08,467 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:08,468 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,564 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,641 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,643 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,770 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,771 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:09,774 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:10,893 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:12,462 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-12 12:35:12,464 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "output_filename = \"bnbc-chap1.json\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Success! Your structured data is saved in {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5qhHbR6LyYb",
        "outputId": "39ec0a1b-8c4d-47b2-ef3c-e630b3d92169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Your structured data is saved in bnbc-chap1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWdqL4VtcgbB",
        "outputId": "bee6683f-ccb3-46cd-d6d0-f502546b4082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Config (edit these paths)\n",
        "# ----------------------------\n",
        "INPUT_DOCLING_JSON = \"/content/bnbc-chap1.json\"\n",
        "INPUT_PDF = \"/content/Bangladesh-National-Building-Code-chap1.pdf\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/structured_out\"\n",
        "IMAGES_DIR_NAME = \"images\"\n",
        "DPI = 200\n",
        "IMAGE_FORMAT = \"jpg\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: Docling JSON access\n",
        "# ----------------------------\n",
        "CLAUSE_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)\\s+(.*\\S)\\s*$\")\n",
        "\n",
        "\n",
        "def normalize_ws(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
        "\n",
        "\n",
        "def load_docling_json(path: str) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def resolve_ref(doc: dict, ref: str):\n",
        "    \"\"\"\n",
        "    ref examples:\n",
        "      '#/texts/93'\n",
        "      '#/tables/0'\n",
        "      '#/pictures/2'\n",
        "      '#/groups/5'\n",
        "    \"\"\"\n",
        "    m = re.match(r\"^#/(texts|tables|pictures|groups)/(\\d+)$\", ref)\n",
        "    if not m:\n",
        "        return None, None, None\n",
        "    kind, idx = m.group(1), int(m.group(2))\n",
        "    return kind, idx, doc[kind][idx]\n",
        "\n",
        "\n",
        "def ref_to_index(ref: str) -> int:\n",
        "    # '#/texts/31' -> 31\n",
        "    return int(ref.split(\"/\")[-1])\n",
        "\n",
        "\n",
        "def extract_caption(doc: dict, captions) -> str:\n",
        "    \"\"\"\n",
        "    captions is typically a list like [{'$ref': '#/texts/93'}]\n",
        "    \"\"\"\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for c in captions:\n",
        "        r = c.get(\"$ref\")\n",
        "        if not r:\n",
        "            continue\n",
        "        kind, _, obj = resolve_ref(doc, r)\n",
        "        if kind == \"texts\":\n",
        "            t = normalize_ws(obj.get(\"text\") or \"\")\n",
        "            if t:\n",
        "                parts.append(t)\n",
        "    return \" \".join(parts).strip()\n",
        "\n",
        "\n",
        "def walk_body_in_reading_order(doc: dict):\n",
        "    \"\"\"\n",
        "    Body children contains a mixture of texts/tables/pictures and also '#/groups/*'.\n",
        "    We DFS into groups so we don't miss content.\n",
        "    \"\"\"\n",
        "    body = doc.get(\"body\") or {}\n",
        "    stack = []\n",
        "\n",
        "    def push_children(children):\n",
        "        for child in reversed(children or []):\n",
        "            stack.append(child)\n",
        "\n",
        "    push_children(body.get(\"children\"))\n",
        "\n",
        "    while stack:\n",
        "        item = stack.pop()\n",
        "        if not isinstance(item, dict) or \"$ref\" not in item:\n",
        "            continue\n",
        "\n",
        "        ref = item[\"$ref\"]\n",
        "        kind, _, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"groups\":\n",
        "            push_children(obj.get(\"children\"))\n",
        "            continue\n",
        "\n",
        "        yield ref\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Tables (FIXED)\n",
        "# ----------------------------\n",
        "def table_rows_robust(table_obj: dict):\n",
        "    \"\"\"\n",
        "    Robustly converts Docling tables into rows[][].\n",
        "\n",
        "    Handles:\n",
        "      A) data[\"grid\"][r][c] is a dict with \"text\"\n",
        "      B) data[\"grid\"][r][c] is an int index into data[\"table_cells\"]\n",
        "      C) missing/empty grid -> reconstruct from table_cells span metadata\n",
        "    \"\"\"\n",
        "    data = table_obj.get(\"data\") or {}\n",
        "    grid = data.get(\"grid\") or []\n",
        "    table_cells = data.get(\"table_cells\") or []\n",
        "\n",
        "    num_rows = int(data.get(\"num_rows\") or (len(grid) if grid else 0))\n",
        "    num_cols = int(data.get(\"num_cols\") or (len(grid[0]) if grid and grid[0] else 0))\n",
        "\n",
        "    # Case A: grid holds dict cells directly\n",
        "    if grid and grid[0] and isinstance(grid[0][0], dict):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for cell in r:\n",
        "                if isinstance(cell, dict):\n",
        "                    row.append(normalize_ws(cell.get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    # Case B: grid holds ints referencing table_cells\n",
        "    if grid and grid[0] and isinstance(grid[0][0], int):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for idx in r:\n",
        "                if isinstance(idx, int) and 0 <= idx < len(table_cells):\n",
        "                    row.append(normalize_ws(table_cells[idx].get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    # Case C: reconstruct from table_cells span metadata\n",
        "    mat = [[\"\" for _ in range(num_cols)] for _ in range(num_rows)]\n",
        "    for cell in table_cells:\n",
        "        txt = normalize_ws(cell.get(\"text\", \"\"))\n",
        "        r0 = cell.get(\"start_row_offset_idx\", 0)\n",
        "        r1 = cell.get(\"end_row_offset_idx\", r0 + 1)\n",
        "        c0 = cell.get(\"start_col_offset_idx\", 0)\n",
        "        c1 = cell.get(\"end_col_offset_idx\", c0 + 1)\n",
        "        for rr in range(r0, r1):\n",
        "            for cc in range(c0, c1):\n",
        "                if 0 <= rr < num_rows and 0 <= cc < num_cols:\n",
        "                    mat[rr][cc] = txt\n",
        "\n",
        "    return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": mat}\n",
        "\n",
        "\n",
        "def build_tables_json(doc: dict) -> list:\n",
        "    out = []\n",
        "    for i, tbl in enumerate(doc.get(\"tables\", [])):\n",
        "        provs = tbl.get(\"prov\") or []\n",
        "        page_no = provs[0].get(\"page_no\") if provs else None\n",
        "        bbox = provs[0].get(\"bbox\") if provs else None\n",
        "        caption = extract_caption(doc, tbl.get(\"captions\"))\n",
        "\n",
        "        t = table_rows_robust(tbl)  # <-- fixed\n",
        "\n",
        "        out.append(\n",
        "            {\n",
        "                \"table_id\": f\"table_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"num_rows\": t[\"num_rows\"],\n",
        "                \"num_cols\": t[\"num_cols\"],\n",
        "                \"rows\": t[\"rows\"],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Images (figures) via PDF crop\n",
        "# ----------------------------\n",
        "def bbox_to_fitz_rect(bbox: dict, page_height: float) -> fitz.Rect | None:\n",
        "    \"\"\"\n",
        "    Docling bbox has coord_origin, often 'BOTTOMLEFT'.\n",
        "    PyMuPDF uses TOPLEFT origin.\n",
        "    \"\"\"\n",
        "    l = bbox.get(\"l\")\n",
        "    t = bbox.get(\"t\")\n",
        "    r = bbox.get(\"r\")\n",
        "    b = bbox.get(\"b\")\n",
        "    if None in (l, t, r, b):\n",
        "        return None\n",
        "\n",
        "    origin = (bbox.get(\"coord_origin\") or \"TOPLEFT\").upper()\n",
        "\n",
        "    if origin == \"BOTTOMLEFT\":\n",
        "        # Convert y coords from bottom-origin to top-origin\n",
        "        y0 = page_height - t\n",
        "        y1 = page_height - b\n",
        "    else:\n",
        "        y0 = t\n",
        "        y1 = b\n",
        "\n",
        "    x0, x1 = sorted([l, r])\n",
        "    y0, y1 = sorted([y0, y1])\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "\n",
        "def extract_figures_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    images_meta = []\n",
        "\n",
        "    for i, pic in enumerate(doc.get(\"pictures\", [])):\n",
        "        provs = pic.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]  # usually only one\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        page_rect = page.rect\n",
        "\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page_rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page_rect  # clamp to page\n",
        "        caption = extract_caption(doc, pic.get(\"captions\"))\n",
        "\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        fname = f\"figure_{i:04d}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        images_meta.append(\n",
        "            {\n",
        "                \"figure_id\": f\"figure_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return images_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Clause tree\n",
        "# ----------------------------\n",
        "def build_clause_tree(doc: dict):\n",
        "    \"\"\"\n",
        "    Builds nodes keyed by clause id (e.g., '5.5', '5.5.1', '5.5.1.1').\n",
        "    Figures are stored ONLY as captions (plus id).\n",
        "    Tables are referenced (plus caption) but table contents are in a separate JSON.\n",
        "    \"\"\"\n",
        "    nodes = {}\n",
        "    root_id = \"ROOT\"\n",
        "    nodes[root_id] = {\"id\": root_id, \"title\": \"\", \"text\": \"\", \"children\": [], \"tables\": [], \"figures\": []}\n",
        "    current_id = root_id\n",
        "\n",
        "    def ensure_node(cid: str):\n",
        "        if cid not in nodes:\n",
        "            nodes[cid] = {\"id\": cid, \"title\": \"\", \"text\": \"\", \"children\": [], \"tables\": [], \"figures\": []}\n",
        "\n",
        "    def parent_id(cid: str) -> str:\n",
        "        parts = cid.split(\".\")\n",
        "        return root_id if len(parts) <= 1 else \".\".join(parts[:-1])\n",
        "\n",
        "    def add_child(pid: str, cid: str):\n",
        "        if cid not in nodes[pid][\"children\"]:\n",
        "            nodes[pid][\"children\"].append(cid)\n",
        "\n",
        "    for ref in walk_body_in_reading_order(doc):\n",
        "        kind, idx, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"texts\":\n",
        "            text = (obj.get(\"text\") or \"\").strip()\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            m = CLAUSE_RE.match(text)\n",
        "            if m:\n",
        "                cid = m.group(1)\n",
        "                rest = m.group(2).strip()\n",
        "\n",
        "                ensure_node(cid)\n",
        "                pid = parent_id(cid)\n",
        "                ensure_node(pid)\n",
        "                add_child(pid, cid)\n",
        "\n",
        "                depth = len(cid.split(\".\"))\n",
        "\n",
        "                # Heuristic:\n",
        "                # - depth <= 3: likely headings (section/subsection), store rest as title\n",
        "                # - depth >= 4: likely “numbered requirement sentence”, store rest as body text\n",
        "                if depth <= 3:\n",
        "                    if not nodes[cid][\"title\"]:\n",
        "                        nodes[cid][\"title\"] = rest\n",
        "                else:\n",
        "                    nodes[cid][\"text\"] += rest + \"\\n\"\n",
        "\n",
        "                current_id = cid\n",
        "            else:\n",
        "                nodes[current_id][\"text\"] += text + \"\\n\"\n",
        "\n",
        "        elif kind == \"tables\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"tables\"].append({\"table_id\": f\"table_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "        elif kind == \"pictures\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"figures\"].append({\"figure_id\": f\"figure_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "    # cleanup whitespace\n",
        "    for nid in nodes:\n",
        "        nodes[nid][\"text\"] = nodes[nid][\"text\"].strip()\n",
        "\n",
        "    return {\"root\": root_id, \"nodes\": nodes}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Optional: retrieval helper\n",
        "# ----------------------------\n",
        "def collect_text_recursive(structured: dict, clause_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns clause text + all descendants' text (useful when user requests a whole section).\n",
        "    \"\"\"\n",
        "    nodes = structured[\"nodes\"]\n",
        "    if clause_id not in nodes:\n",
        "        return \"\"\n",
        "\n",
        "    n = nodes[clause_id]\n",
        "    chunks = []\n",
        "\n",
        "    if clause_id != \"ROOT\" and n[\"title\"]:\n",
        "        chunks.append(f\"{clause_id} {n['title']}\".strip())\n",
        "    if n[\"text\"]:\n",
        "        chunks.append(n[\"text\"])\n",
        "\n",
        "    for fig in n.get(\"figures\", []):\n",
        "        if fig.get(\"caption\"):\n",
        "            chunks.append(fig[\"caption\"])\n",
        "    for tbl in n.get(\"tables\", []):\n",
        "        if tbl.get(\"caption\"):\n",
        "            chunks.append(tbl[\"caption\"])\n",
        "\n",
        "    for child in n.get(\"children\", []):\n",
        "        child_txt = collect_text_recursive(structured, child)\n",
        "        if child_txt:\n",
        "            chunks.append(child_txt)\n",
        "\n",
        "    return \"\\n\".join(chunks).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    out_base = Path(OUTPUT_DIR)\n",
        "    out_base.mkdir(parents=True, exist_ok=True)\n",
        "    images_dir = out_base / IMAGES_DIR_NAME\n",
        "\n",
        "    doc = load_docling_json(INPUT_DOCLING_JSON)\n",
        "\n",
        "    # 1) Clause tree (with figure captions only)\n",
        "    clauses_struct = build_clause_tree(doc)\n",
        "\n",
        "    # 2) Tables as rows (FIXED)\n",
        "    tables_struct = build_tables_json(doc)\n",
        "\n",
        "    # 3) Figures as JPG + metadata\n",
        "    images_struct = extract_figures_from_pdf(\n",
        "        doc=doc,\n",
        "        pdf_path=INPUT_PDF,\n",
        "        out_dir=str(images_dir),\n",
        "        dpi=DPI,\n",
        "        image_format=IMAGE_FORMAT,\n",
        "    )\n",
        "\n",
        "    # Save outputs\n",
        "    clauses_path = out_base / \"structured_clauses.json\"\n",
        "    tables_path = out_base / \"structured_tables.json\"\n",
        "    images_path = out_base / \"structured_images.json\"\n",
        "\n",
        "    clauses_path.write_text(json.dumps(clauses_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    tables_path.write_text(json.dumps(tables_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    images_path.write_text(json.dumps(images_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # Example usage: get all text for a section including subclauses\n",
        "    example_section = next((k for k in clauses_struct[\"nodes\"].keys() if k not in (\"ROOT\",) and k.count(\".\") == 1), None)\n",
        "    if example_section:\n",
        "        print(\"\\nExample section:\", example_section)\n",
        "        print(collect_text_recursive(clauses_struct, example_section)[:800], \"...\\n\")\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    print(\" -\", clauses_path)\n",
        "    print(\" -\", tables_path)\n",
        "    print(\" -\", images_path)\n",
        "    print(\"Images saved under:\", images_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSaeLuiU9VM",
        "outputId": "86ea4d90-5989-4edd-efa1-ba1d07b56bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example section: 1.1\n",
            "1.1 Scope\n",
            "This  Part  of  the  Code  puts  forward  classification  of  buildings  based  on  occupancy  or nature  of  use  and  deals  with  the  general  and  specific  requirements  of  each  of  the occupancy  groups.  Fire  resistance  requirements  are  expressed  in  terms  of  type  of construction which shall conform to the specified fire-resistive properties. ...\n",
            "\n",
            "Saved:\n",
            " - /content/structured_out/structured_clauses.json\n",
            " - /content/structured_out/structured_tables.json\n",
            " - /content/structured_out/structured_images.json\n",
            "Images saved under: /content/structured_out/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRQtuEAZcmzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}