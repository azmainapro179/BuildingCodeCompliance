{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GpNIa_LLRT",
        "outputId": "06ae2484-467e-4f9f-8bab-e6cb61102fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.70.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.3)\n",
            "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading docling_core-2.61.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.11.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<6.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2026.1.4)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.4.1)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (6.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.12.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.9.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.26.0)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter<0.27.0,>=0.25.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-c>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tree-sitter-javascript>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tree-sitter-typescript>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.6)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.0+cu126)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: opencv_python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.13.0.90)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading docling-2.70.0-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.9/299.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.61.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.11.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.6.0-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (635 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=6bba2cc829fd8ba907bdf1affcfef7390c9f717c6b62b45b9bfa55f4afdba4d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, filetype, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-c, tree-sitter, python-docx, pypdfium2, pyclipper, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.21.1\n",
            "    Uninstalling typer-0.21.1:\n",
            "      Successfully uninstalled typer-0.21.1\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.70.0 docling-core-2.61.0 docling-ibm-models-3.11.0 docling-parse-4.7.3 faker-40.1.2 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.2 mpire-2.10.2 polyfactory-3.2.0 pyclipper-1.4.0 pylatexenc-2.10 pypdfium2-5.3.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.6.0 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install img2pdf\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import img2pdf\n",
        "import os\n",
        "import shutil\n",
        "import gc  # Garbage collector\n",
        "\n",
        "# Config\n",
        "input_pdf_path = \"/content/bnbc6_chap1.pdf\"\n",
        "output_pdf_path = \"/content/bnbc6_chap1_flat.pdf\"\n",
        "temp_dir = \"/content/temp_flatten_pages\"\n",
        "DPI = 400  # Reduced from 300 to 200 (Sufficient for OCR, saves 50% RAM)\n",
        "\n",
        "# 1. Setup temp directory\n",
        "if os.path.exists(temp_dir):\n",
        "    shutil.rmtree(temp_dir)\n",
        "os.makedirs(temp_dir)\n",
        "\n",
        "print(f\"Phase 1: Rasterizing pages to disk (DPI={DPI})...\")\n",
        "\n",
        "# Open PDF\n",
        "doc = fitz.open(input_pdf_path)\n",
        "image_paths = []\n",
        "\n",
        "for i, page in enumerate(doc):\n",
        "    # Render page to image\n",
        "    pix = page.get_pixmap(dpi=DPI)\n",
        "\n",
        "    # Save to DISK immediately (keeps RAM clean)\n",
        "    image_filename = f\"{temp_dir}/page_{i:04d}.jpg\"\n",
        "    pix.save(image_filename)\n",
        "    image_paths.append(image_filename)\n",
        "\n",
        "    # Critical: Delete objects and force garbage collection to free RAM\n",
        "    del pix\n",
        "    gc.collect()\n",
        "\n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"  Saved page {i + 1}/{len(doc)}\")\n",
        "\n",
        "doc.close()\n",
        "print(\"Phase 1 Complete. Images saved to disk.\")\n",
        "\n",
        "# 2. Combine images into PDF using img2pdf (Streams directly from disk)\n",
        "print(\"Phase 2: Stitching images back into PDF...\")\n",
        "\n",
        "with open(output_pdf_path, \"wb\") as f:\n",
        "    f.write(img2pdf.convert(image_paths))\n",
        "\n",
        "# 3. Cleanup\n",
        "shutil.rmtree(temp_dir)\n",
        "print(f\"Success! Flattened PDF saved at: {output_pdf_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWFNNgKdeHoZ",
        "outputId": "505b3dfa-0325-4bba-dd06-4832fe66be79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.7)\n",
            "Requirement already satisfied: img2pdf in /usr/local/lib/python3.12/dist-packages (0.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from img2pdf) (11.3.0)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.12/dist-packages (from img2pdf) (10.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pikepdf->img2pdf) (1.3.1)\n",
            "Requirement already satisfied: lxml>=4.8 in /usr/local/lib/python3.12/dist-packages (from pikepdf->img2pdf) (6.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pikepdf->img2pdf) (25.0)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->pikepdf->img2pdf) (2.0.1)\n",
            "Phase 1: Rasterizing pages to disk (DPI=400)...\n",
            "  Saved page 5/37\n",
            "  Saved page 10/37\n",
            "  Saved page 15/37\n",
            "  Saved page 20/37\n",
            "  Saved page 25/37\n",
            "  Saved page 30/37\n",
            "  Saved page 35/37\n",
            "Phase 1 Complete. Images saved to disk.\n",
            "Phase 2: Stitching images back into PDF...\n",
            "Success! Flattened PDF saved at: /content/bnbc6_chap1_flat.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnpkfsE8LR6a",
        "outputId": "21d30bd7-d2e9-4bc9-8355-53c74246aaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing flattened PDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2026-01-30 12:54:52,228 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,233 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,334 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,341 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,784 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,786 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,790 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,791 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,940 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:52,941 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:53,112 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-30 12:54:53,114 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First text item: ,,,O>\n"
          ]
        }
      ],
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "\n",
        "# USE THE FLATTENED FILE\n",
        "source = \"/content/bnbc6_chap1_flat.pdf\"\n",
        "\n",
        "# Standard OCR options are sufficient now because there is NO text layer to confuse it\n",
        "pipeline_options = PdfPipelineOptions()\n",
        "pipeline_options.do_ocr = True\n",
        "pipeline_options.do_formula_enrichment = True\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Processing flattened PDF...\")\n",
        "result = converter.convert(source)\n",
        "\n",
        "# Export and Verify\n",
        "structured_json = result.document.export_to_dict()\n",
        "\n",
        "# This should now print the actual symbol (e.g., 'D', 'E') instead of GLYPH\n",
        "first_text = structured_json[\"texts\"][0][\"text\"]\n",
        "print(\"First text item:\", first_text)\n",
        "\n",
        "# Save for your parser\n",
        "import json\n",
        "with open(\"bnbc6_chap1.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_json, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5qhHbR6LyYb",
        "outputId": "6f648cc8-f97c-4b94-d9ee-0755a953b44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Your structured data is saved in bnbc6_chap1.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "output_filename = \"bnbc6_chap1.json\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Success! Your structured data is saved in {output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWdqL4VtcgbB",
        "outputId": "4f1a78d2-2d52-4fa8-b523-59d763f50d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.7\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSaeLuiU9VM",
        "outputId": "6955419b-474f-4298-b07f-c44af3985d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG build_tables_json: mm pages = [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]\n",
            "DEBUG: Early return - page_no=2, in_mm=False\n",
            "DEBUG: Early return - page_no=3, in_mm=False\n",
            "DEBUG: page=4, table_bbox={'l': 129.060791015625, 't': 665.3757400512695, 'r': 484.0224914550781, 'b': 157.935546875, 'coord_origin': 'BOTTOMLEFT'}\n",
            "DEBUG: span=46.30645243326819, coord_origin=BOTTOMLEFT, top_high=True, table_phys_top=665.3757400512695\n",
            "Saved:\n",
            " - /content/structured_out/structured_clauses.json\n",
            " - /content/structured_out/structured_tables.json\n",
            " - /content/structured_out/structured_images.json\n",
            " - /content/structured_out/structured_equations.json\n",
            "\n",
            "Counts:\n",
            " clauses: 79\n",
            " tables: 6\n",
            " figures: 1\n",
            " equations: 1\n",
            " list blocks: 37\n",
            " list items: 109\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# ----------------------------\n",
        "# Config (edit these paths)\n",
        "# ----------------------------\n",
        "INPUT_DOCLING_JSON = \"/content/bnbc6_chap1.json\"\n",
        "INPUT_PDF = \"/content/bnbc6_chap1_flat.pdf\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/structured_out\"\n",
        "IMAGES_DIR_NAME = \"images\"\n",
        "EQUATIONS_DIR_NAME = \"equations\"\n",
        "DPI = 200\n",
        "IMAGE_FORMAT = \"jpg\"\n",
        "\n",
        "# If Docling formula enrichment produces spaced-out LaTeX (each char separated by spaces),\n",
        "# enable this heuristic de-spacing fix.\n",
        "FIX_SPACED_LATEX = True\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: Docling JSON access\n",
        "# ----------------------------\n",
        "CLAUSE_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)\\s+(.*\\S)\\s*$\")\n",
        "\n",
        "# List marker patterns (start-of-line)\n",
        "PAREN_MARKER_RE = re.compile(r\"^\\s*\\(\\s*([A-Za-z0-9]+|[ivxlcdmIVXLCDM]+)\\s*\\)\\s+\")\n",
        "SIMPLE_MARKER_RE = re.compile(r\"^\\s*([A-Za-z]|\\d+|[ivxlcdmIVXLCDM]+)([\\.|\\)])\\s+\")\n",
        "BULLET_MARKER_RE = re.compile(r\"^\\s*([•\\-*–—])\\s+\")\n",
        "\n",
        "# Inline markers inside a single paragraph, e.g. \"... requirements: (1) ... (2) ... (3) ...\"\n",
        "# We only treat a parenthesized token as a marker if it's preceded by start/whitespace/:/; and followed by space.\n",
        "INLINE_PAREN_MARKER_RE = re.compile(r\"(?:(?<=^)|(?<=[\\s:;]))\\(\\s*([0-9]+|[A-Za-z]|[ivxlcdmIVXLCDM]+)\\s*\\)\\s+\")\n",
        "\n",
        "# Strong table caption indicator: \"Table <no>: <text>\"\n",
        "TABLE_CAPTION_STRONG_RE = re.compile(r\"^\\s*Table\\s+\\d+(?:\\.\\d+)*\\s*:\\s*\\S\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "\n",
        "def normalize_ws(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
        "\n",
        "\n",
        "def load_docling_json(path: str) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def resolve_ref(doc: dict, ref: str):\n",
        "    \"\"\"\n",
        "    ref examples:\n",
        "      '#/texts/93'\n",
        "      '#/tables/0'\n",
        "      '#/pictures/2'\n",
        "      '#/groups/5'\n",
        "    \"\"\"\n",
        "    m = re.match(r\"^#/(texts|tables|pictures|groups)/(\\d+)$\", ref)\n",
        "    if not m:\n",
        "        return None, None, None\n",
        "    kind, idx = m.group(1), int(m.group(2))\n",
        "    return kind, idx, doc[kind][idx]\n",
        "\n",
        "\n",
        "def extract_caption(doc: dict, captions) -> str:\n",
        "    \"\"\"Extract concatenated caption text from refs like [{'$ref':'#/texts/93'}].\"\"\"\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for c in captions:\n",
        "        r = c.get(\"$ref\")\n",
        "        if not r:\n",
        "            continue\n",
        "        kind, _, obj = resolve_ref(doc, r)\n",
        "        if kind == \"texts\":\n",
        "            t = normalize_ws(obj.get(\"text\") or \"\")\n",
        "            if t:\n",
        "                parts.append(t)\n",
        "    return \" \".join(parts).strip()\n",
        "\n",
        "\n",
        "def walk_body_in_reading_order(doc: dict):\n",
        "    \"\"\"\n",
        "    Body children contains a mixture of texts/tables/pictures and also '#/groups/*'.\n",
        "    We DFS into groups so we don't miss content.\n",
        "    \"\"\"\n",
        "    body = doc.get(\"body\") or {}\n",
        "    stack = []\n",
        "\n",
        "    def push_children(children):\n",
        "        for child in reversed(children or []):\n",
        "            stack.append(child)\n",
        "\n",
        "    push_children(body.get(\"children\"))\n",
        "\n",
        "    while stack:\n",
        "        item = stack.pop()\n",
        "        if not isinstance(item, dict) or \"$ref\" not in item:\n",
        "            continue\n",
        "\n",
        "        ref = item[\"$ref\"]\n",
        "        kind, _, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"groups\":\n",
        "            push_children(obj.get(\"children\"))\n",
        "            continue\n",
        "\n",
        "        yield ref\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Generic point parsing utilities\n",
        "# ----------------------------\n",
        "def leading_indent_spaces(s: str) -> int:\n",
        "    if not s:\n",
        "        return 0\n",
        "    n = 0\n",
        "    for ch in s:\n",
        "        if ch == \" \":\n",
        "            n += 1\n",
        "        elif ch == \"\\t\":\n",
        "            n += 4\n",
        "        else:\n",
        "            break\n",
        "    return n\n",
        "\n",
        "\n",
        "def parse_list_marker(raw_text: str, marker_field: str = \"\", allow_inline_start: bool = True):\n",
        "    \"\"\"\n",
        "    Returns (marker, content, indent_spaces) if this looks like a point item; else (None, None, None).\n",
        "\n",
        "    Priority:\n",
        "      1) Docling-provided marker_field (e.g., '1.' or '*')\n",
        "      2) '(a) ...', '(1) ...', '(i) ...' at START of text\n",
        "      3) 'a) ...', '1. ...', 'i) ...' at START of text\n",
        "      4) bullet char at START of text\n",
        "    \"\"\"\n",
        "    if raw_text is None:\n",
        "        return None, None, None\n",
        "\n",
        "    indent = leading_indent_spaces(raw_text)\n",
        "    s = raw_text.lstrip(\"\\t \").rstrip()\n",
        "\n",
        "    mf = (marker_field or \"\").strip()\n",
        "    if mf:\n",
        "        return mf, normalize_ws(s), indent\n",
        "\n",
        "    if not allow_inline_start or not s:\n",
        "        return None, None, None\n",
        "\n",
        "    m = PAREN_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"({m.group(1)})\"\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    m = SIMPLE_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"{m.group(1)}{m.group(2)}\"\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    m = BULLET_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = m.group(1)\n",
        "        return mk, normalize_ws(s[m.end():]), indent\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def split_inline_enumeration(raw_text: str):\n",
        "    \"\"\"\n",
        "    If raw_text contains multiple inline markers like:\n",
        "      \"Intro: (1) aaa (2) bbb (3) ccc\"\n",
        "    return:\n",
        "      intro=\"Intro:\" and items=[{\"marker\":\"(1)\",\"text\":\"aaa\"}, ...]\n",
        "    Otherwise returns (None, None).\n",
        "    \"\"\"\n",
        "    if not raw_text:\n",
        "        return None, None\n",
        "    s = raw_text.strip()\n",
        "    matches = list(INLINE_PAREN_MARKER_RE.finditer(s))\n",
        "    if len(matches) < 2:\n",
        "        return None, None\n",
        "\n",
        "    intro = s[:matches[0].start()].strip()\n",
        "    items = []\n",
        "    for i, mm in enumerate(matches):\n",
        "        mk = f\"({mm.group(1)})\"\n",
        "        start = mm.end()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(s)\n",
        "        content = normalize_ws(s[start:end]).strip(\" ;\")\n",
        "        if content:\n",
        "            items.append({\"marker\": mk, \"text\": content})\n",
        "    if not items:\n",
        "        return None, None\n",
        "    return intro, items\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Repeating header/footer (boilerplate) detection\n",
        "# ----------------------------\n",
        "NOISE_CANONICALS = set()\n",
        "\n",
        "CAPTION_TEXT_KEYS = set()\n",
        "\n",
        "def _make_text_key_from_obj(obj: dict):\n",
        "    \"\"\"Stable-ish key for a text object so we can later skip it in clause text if it was used as a table caption.\"\"\"\n",
        "    raw = normalize_ws(obj.get(\"text\") or \"\")\n",
        "    if not raw:\n",
        "        return None\n",
        "    prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "    page_no = prov0.get(\"page_no\")\n",
        "    bb = prov0.get(\"bbox\") or {}\n",
        "    if page_no is None or not isinstance(bb, dict):\n",
        "        return None\n",
        "    l = bb.get(\"l\"); t = bb.get(\"t\"); r = bb.get(\"r\"); b = bb.get(\"b\")\n",
        "    if None in (l, t, r, b):\n",
        "        return None\n",
        "    return (\n",
        "        int(page_no),\n",
        "        canonicalize_repeat_text(raw),\n",
        "        round(float(l), 1),\n",
        "        round(float(t), 1),\n",
        "        round(float(r), 1),\n",
        "        round(float(b), 1),\n",
        "    )\n",
        "\n",
        "def is_caption_text_obj(obj: dict) -> bool:\n",
        "    k = _make_text_key_from_obj(obj)\n",
        "    return k is not None and k in CAPTION_TEXT_KEYS\n",
        "\n",
        "def _unicode_digit_mask(s: str) -> str:\n",
        "    \"\"\"Replace any Unicode decimal digit (Latin/Bengali/Arabic-Indic/etc.) with '#'.\"\"\"\n",
        "    out = []\n",
        "    for ch in (s or \"\"):\n",
        "        out.append(\"#\" if unicodedata.category(ch) == \"Nd\" else ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def canonicalize_repeat_text(s: str) -> str:\n",
        "    \"\"\"Canonical form for comparing repeated header/footer strings across pages.\"\"\"\n",
        "    s = normalize_ws(s)\n",
        "    s = _unicode_digit_mask(s)\n",
        "    # Keep letters/digits/underscore across scripts; drop punctuation\n",
        "    s = re.sub(r\"[^\\w\\s#]\", \" \", s, flags=re.UNICODE)\n",
        "    s = normalize_ws(s).lower()\n",
        "    # collapse repeated masked digits\n",
        "    s = re.sub(r\"(#\\s*){2,}\", \"#\", s)\n",
        "    return s\n",
        "\n",
        "def _percentile(vals, p: float):\n",
        "    if not vals:\n",
        "        return None\n",
        "    vals = sorted(vals)\n",
        "    k = int(round((p / 100.0) * (len(vals) - 1)))\n",
        "    k = max(0, min(len(vals) - 1, k))\n",
        "    return vals[k]\n",
        "\n",
        "def detect_repeating_headers_footers(doc: dict,\n",
        "                                     min_page_fraction: float = 0.6,\n",
        "                                     band_percentile: float = 8.0) -> set:\n",
        "    \"\"\"\n",
        "    Learn boilerplate (headers/footers) generically:\n",
        "      - take text items in the top/bottom bands of each page (by bbox.t distribution)\n",
        "      - canonicalize (mask digits, normalize)\n",
        "      - mark items repeated across many distinct pages\n",
        "    \"\"\"\n",
        "    page_ts = defaultdict(list)\n",
        "    items = []\n",
        "\n",
        "    for obj in doc.get(\"texts\", []):\n",
        "        raw = (obj.get(\"text\") or \"\").strip()\n",
        "        if not raw:\n",
        "            continue\n",
        "        prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "        page_no = prov0.get(\"page_no\")\n",
        "        bbox = prov0.get(\"bbox\") or {}\n",
        "        if page_no is None or not isinstance(bbox, dict):\n",
        "            continue\n",
        "        t = bbox.get(\"t\")\n",
        "        if t is None:\n",
        "            continue\n",
        "        page_ts[page_no].append(t)\n",
        "        items.append((page_no, t, raw))\n",
        "\n",
        "    pages = sorted(page_ts.keys())\n",
        "    if not pages:\n",
        "        return set()\n",
        "\n",
        "    top_cut = {}\n",
        "    bot_cut = {}\n",
        "    for p in pages:\n",
        "        ts = page_ts[p]\n",
        "        top_cut[p] = _percentile(ts, 100.0 - band_percentile)\n",
        "        bot_cut[p] = _percentile(ts, band_percentile)\n",
        "\n",
        "    cand_pages = defaultdict(set)\n",
        "    for page_no, t, raw in items:\n",
        "        tc = top_cut.get(page_no)\n",
        "        bc = bot_cut.get(page_no)\n",
        "        if tc is None or bc is None:\n",
        "            continue\n",
        "        # candidate if it lives in either extreme band on that page\n",
        "        if t >= tc or t <= bc:\n",
        "            c = canonicalize_repeat_text(raw)\n",
        "            if 5 <= len(c) <= 160:\n",
        "                cand_pages[c].add(page_no)\n",
        "\n",
        "    n_pages = len(pages)\n",
        "    need = max(3, int(round(min_page_fraction * n_pages)))\n",
        "    return {c for c, ps in cand_pages.items() if len(ps) >= need}\n",
        "\n",
        "\n",
        "def is_noise_text(s: str) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristic to ignore headers/footers/page numbers that can appear between list items.\n",
        "    This stays generic (no language-specific keywords):\n",
        "      - learned repeating boilerplate via detect_repeating_headers_footers()\n",
        "      - plus light heuristics for page numbers / punctuation soup\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return True\n",
        "    t = s.strip()\n",
        "    if not t:\n",
        "        return True\n",
        "\n",
        "    if NOISE_CANONICALS and canonicalize_repeat_text(t) in NOISE_CANONICALS:\n",
        "        return True\n",
        "\n",
        "    if re.fullmatch(r\"\\d{3,6}\", t):  # page numbers like 3235 (Latin digits)\n",
        "        return True\n",
        "    if len(t) <= 12 and sum(ch.isalpha() for ch in t) < 2:\n",
        "        return True\n",
        "    # very punctuation-heavy short lines\n",
        "    if len(t) <= 25 and sum(ch.isalnum() for ch in t) / max(1, len(t)) < 0.35:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _reading_order_key(block: dict):\n",
        "    \"\"\"Sort by page then top-to-bottom then left-to-right using bbox.\"\"\"\n",
        "    page_no = block.get(\"page_no\")\n",
        "    bbox = block.get(\"bbox\") or {}\n",
        "    l = bbox.get(\"l\") if isinstance(bbox, dict) else None\n",
        "    t = bbox.get(\"t\") if isinstance(bbox, dict) else None\n",
        "    origin = (bbox.get(\"coord_origin\") or \"TOPLEFT\").upper() if isinstance(bbox, dict) else \"TOPLEFT\"\n",
        "\n",
        "    page_key = page_no if page_no is not None else 10**9\n",
        "    y_key = 0 if t is None else ((-t) if origin == \"BOTTOMLEFT\" else t)\n",
        "    x_key = l if l is not None else 0\n",
        "    return (page_key, y_key, x_key)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Smart nesting (indent + marker kind + lookahead)\n",
        "# ----------------------------\n",
        "def _is_roman(s: str) -> bool:\n",
        "    if not s:\n",
        "        return False\n",
        "    s = s.strip().lower()\n",
        "    return bool(re.fullmatch(r\"[ivxlcdm]+\", s))\n",
        "\n",
        "\n",
        "def marker_kind(marker: str) -> str:\n",
        "    if not marker:\n",
        "        return \"other\"\n",
        "    m = marker.strip()\n",
        "\n",
        "    if m in {\"•\", \"-\", \"*\", \"–\", \"—\"}:\n",
        "        return \"bullet\"\n",
        "\n",
        "    core = re.sub(r\"^[\\(\\[]\\s*\", \"\", m)\n",
        "    core = re.sub(r\"\\s*[\\)\\]]$\", \"\", core)\n",
        "    core = re.sub(r\"[\\.)]$\", \"\", core).strip()\n",
        "\n",
        "    if core.isdigit():\n",
        "        return \"num\"\n",
        "    if len(core) == 1 and core.isalpha():\n",
        "        return \"alpha\"\n",
        "    if _is_roman(core):\n",
        "        return \"roman\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "KIND_ORDER = {\"num\": 0, \"alpha\": 1, \"roman\": 2, \"bullet\": 3, \"other\": 3}\n",
        "\n",
        "\n",
        "def _nesting_trigger_text(s: str) -> bool:\n",
        "    \"\"\"Non-hardcoded cue: a colon usually introduces a sublist.\"\"\"\n",
        "    return bool(s and s.strip().endswith(\":\"))\n",
        "\n",
        "\n",
        "def _will_return_to_kind(flat_items, start_idx: int, target_kind: str) -> bool:\n",
        "    \"\"\"Lookahead: if we later return to target_kind, items in between are likely a sublist.\"\"\"\n",
        "    for j in range(start_idx + 1, len(flat_items)):\n",
        "        k = marker_kind(flat_items[j].get(\"marker\", \"\"))\n",
        "        if k == target_kind:\n",
        "            return True\n",
        "        if KIND_ORDER.get(k, 3) < KIND_ORDER.get(target_kind, 3):\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "\n",
        "def _marker_num_value(marker: str):\n",
        "    \"\"\"Return integer value for numeric markers like '(8)' or '8.'; else None.\"\"\"\n",
        "    if not marker:\n",
        "        return None\n",
        "    m = marker.strip()\n",
        "    core = re.sub(r\"^[\\(\\[]\\s*\", \"\", m)\n",
        "    core = re.sub(r\"\\s*[\\)\\]]$\", \"\", core)\n",
        "    core = re.sub(r\"[.)]$\", \"\", core).strip()\n",
        "    return int(core) if core.isdigit() else None\n",
        "\n",
        "\n",
        "def nest_list_items_smart(flat_items):\n",
        "    \"\"\"\n",
        "    Build a nested list tree using:\n",
        "      1) indentation (from bbox.l when available) - normalized per page to avoid page-to-page x-shifts\n",
        "      2) marker-kind hierarchy + ':' cue + lookahead return\n",
        "\n",
        "    Fix: avoid accidental nesting across page breaks for same-level numeric sequences\n",
        "         (e.g., (8) at end of a page and (9) at top of next page).\n",
        "    \"\"\"\n",
        "    root = []\n",
        "    stack = []  # entries: {\"indent\": float, \"kind\": str, \"order\": int, \"node\": dict, \"page_no\": int}\n",
        "\n",
        "    for i, it in enumerate(flat_items):\n",
        "        node = {\"marker\": it.get(\"marker\", \"\"), \"text\": it.get(\"text\", \"\"), \"children\": []}\n",
        "        # Use normalized indent when available (per-page baseline removed)\n",
        "        ind = float(it.get(\"indent_norm\", it.get(\"indent\") or 0))\n",
        "        page_no = it.get(\"page_no\", None)\n",
        "\n",
        "        k = marker_kind(node[\"marker\"])\n",
        "        k_order = KIND_ORDER.get(k, 3)\n",
        "\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        prev = stack[-1]\n",
        "        prev_page = prev.get(\"page_no\", None)\n",
        "\n",
        "        # Detect numeric sibling continuation (8 -> 9) to override misleading indent shifts.\n",
        "        prev_num = _marker_num_value(prev[\"node\"].get(\"marker\", \"\"))\n",
        "        curr_num = _marker_num_value(node.get(\"marker\", \"\"))\n",
        "\n",
        "        is_numeric_continuation = (\n",
        "            k == \"num\"\n",
        "            and prev.get(\"kind\") == \"num\"\n",
        "            and prev_num is not None\n",
        "            and curr_num is not None\n",
        "            and curr_num == prev_num + 1\n",
        "        )\n",
        "\n",
        "        # Indent-based nesting when it changes meaningfully.\n",
        "        indent_diff = ind - prev[\"indent\"]\n",
        "        indent_is_informative = abs(indent_diff) >= 5.0  # bbox units are typically points\n",
        "\n",
        "        # ---- FIX #1: across page breaks, ignore indent-based \"nesting\" for numeric continuations\n",
        "        if indent_is_informative and (page_no is not None and prev_page is not None) and (page_no != prev_page) and is_numeric_continuation:\n",
        "            # Force as sibling at the same level as prev.\n",
        "            stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        if indent_is_informative:\n",
        "            while stack and ind <= stack[-1][\"indent\"]:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        # Marker-based inference when indent isn't helpful.\n",
        "        prev_kind = prev[\"kind\"]\n",
        "        prev_order = prev[\"order\"]\n",
        "\n",
        "        prev_introduces_sublist = _nesting_trigger_text(prev[\"node\"].get(\"text\", \"\"))\n",
        "        lookahead_sublist = _will_return_to_kind(flat_items, i, prev_kind)\n",
        "\n",
        "        if (k_order > prev_order) and (prev_introduces_sublist or lookahead_sublist):\n",
        "            prev[\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        if k == prev_kind:\n",
        "            # sibling\n",
        "            stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        if k_order <= prev_order:\n",
        "            while stack and KIND_ORDER.get(stack[-1][\"kind\"], 3) >= k_order:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "            continue\n",
        "\n",
        "        # default: sibling\n",
        "        stack.pop()\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "        else:\n",
        "            stack[-1][\"node\"][\"children\"].append(node)\n",
        "        stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node, \"page_no\": page_no})\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "def blocks_to_text_and_lists(blocks):\n",
        "    \"\"\"\n",
        "    blocks: list of either\n",
        "      {\"kind\":\"text\",\"text\":...}\n",
        "      {\"kind\":\"list_item\",\"marker\":..., \"text\":..., \"indent\":..., \"page_no\":..., \"bbox\":...}\n",
        "\n",
        "    Returns:\n",
        "      text: merged paragraphs (non-list blocks)\n",
        "      lists: list of {\"items\":[nested...]} in reading order\n",
        "\n",
        "    Fix: normalize list-item indents per page before nesting to avoid page-to-page x-offset artifacts.\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "    lists = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(blocks):\n",
        "        b = blocks[i]\n",
        "        if b[\"kind\"] == \"text\":\n",
        "            t = b.get(\"text\", \"\")\n",
        "            if t and not is_noise_text(t):\n",
        "                text_parts.append(t.strip())\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # collect list items, skipping noise text between them\n",
        "        j = i\n",
        "        run = []\n",
        "        while j < len(blocks):\n",
        "            bb = blocks[j]\n",
        "            if bb[\"kind\"] == \"list_item\":\n",
        "                run.append(bb)\n",
        "                j += 1\n",
        "                continue\n",
        "            if bb[\"kind\"] == \"text\" and is_noise_text(bb.get(\"text\", \"\")):\n",
        "                j += 1\n",
        "                continue\n",
        "            break\n",
        "\n",
        "        # Sort by bbox order if we have provenance (fixes e.g., 9, 11, 10).\n",
        "        if any((x.get(\"page_no\") is not None and x.get(\"bbox\")) for x in run):\n",
        "            run = sorted(run, key=_reading_order_key)\n",
        "\n",
        "        # ---- FIX #2: per-page indent normalization\n",
        "        # Some PDFs have different x-offsets per page; compare indents in a normalized coordinate system.\n",
        "        per_page_min = {}\n",
        "        for it in run:\n",
        "            pn = it.get(\"page_no\", None)\n",
        "            ind = it.get(\"indent\", None)\n",
        "            if pn is None or ind is None:\n",
        "                continue\n",
        "            per_page_min[pn] = min(per_page_min.get(pn, float(ind)), float(ind))\n",
        "\n",
        "        for it in run:\n",
        "            pn = it.get(\"page_no\", None)\n",
        "            ind = it.get(\"indent\", None)\n",
        "            if pn is None or ind is None or pn not in per_page_min:\n",
        "                it[\"indent_norm\"] = float(ind or 0)\n",
        "            else:\n",
        "                it[\"indent_norm\"] = float(ind) - float(per_page_min[pn])\n",
        "\n",
        "        nested = nest_list_items_smart(run)\n",
        "        lists.append({\"items\": nested})\n",
        "\n",
        "        i = j\n",
        "\n",
        "    text = \"\\n\".join([p for p in text_parts if p]).strip()\n",
        "    return text, lists\n",
        "\n",
        "def format_list_items(items, level=0):\n",
        "    \"\"\"Render nested list items into plain text (for retrieval display).\"\"\"\n",
        "    out = []\n",
        "    indent = \"  \" * level\n",
        "    for it in items or []:\n",
        "        mk = it.get(\"marker\", \"\")\n",
        "        tx = it.get(\"text\", \"\")\n",
        "        line = f\"{indent}{mk} {tx}\".strip()\n",
        "        if line:\n",
        "            out.append(line)\n",
        "        if it.get(\"children\"):\n",
        "            out.append(format_list_items(it[\"children\"], level + 1))\n",
        "    return \"\\n\".join([x for x in out if x]).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Tables (robust)\n",
        "# ----------------------------\n",
        "def table_rows_robust(table_obj: dict):\n",
        "    \"\"\"\n",
        "    Robustly converts Docling tables into rows[][].\n",
        "\n",
        "    Handles:\n",
        "      A) data[\"grid\"][r][c] is a dict with \"text\"\n",
        "      B) data[\"grid\"][r][c] is an int index into data[\"table_cells\"]\n",
        "      C) missing/empty grid -> reconstruct from table_cells span metadata\n",
        "    \"\"\"\n",
        "    data = table_obj.get(\"data\") or {}\n",
        "    grid = data.get(\"grid\") or []\n",
        "    table_cells = data.get(\"table_cells\") or []\n",
        "\n",
        "    num_rows = int(data.get(\"num_rows\") or (len(grid) if grid else 0))\n",
        "    num_cols = int(data.get(\"num_cols\") or (len(grid[0]) if grid and grid[0] else 0))\n",
        "\n",
        "    if grid and grid[0] and isinstance(grid[0][0], dict):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            rows.append([normalize_ws(cell.get(\"text\", \"\")) if isinstance(cell, dict) else \"\" for cell in r])\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    if grid and grid[0] and isinstance(grid[0][0], int):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for idx in r:\n",
        "                if isinstance(idx, int) and 0 <= idx < len(table_cells):\n",
        "                    row.append(normalize_ws(table_cells[idx].get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    mat = [[\"\" for _ in range(num_cols)] for _ in range(num_rows)]\n",
        "    for cell in table_cells:\n",
        "        txt = normalize_ws(cell.get(\"text\", \"\"))\n",
        "        r0 = cell.get(\"start_row_offset_idx\", 0)\n",
        "        r1 = cell.get(\"end_row_offset_idx\", r0 + 1)\n",
        "        c0 = cell.get(\"start_col_offset_idx\", 0)\n",
        "        c1 = cell.get(\"end_col_offset_idx\", c0 + 1)\n",
        "        for rr in range(r0, r1):\n",
        "            for cc in range(c0, c1):\n",
        "                if 0 <= rr < num_rows and 0 <= cc < num_cols:\n",
        "                    mat[rr][cc] = txt\n",
        "\n",
        "    return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": mat}\n",
        "\n",
        "\n",
        "def build_tables_json(doc: dict):\n",
        "    \"\"\"\n",
        "    Build table JSON with:\n",
        "      1) caption inference (when Docling doesn't attach captions)\n",
        "      2) multi-page continuation merge (same table split across adjacent pages)\n",
        "      3) semantic table_id derived from caption number (e.g., \"6.2.22\")\n",
        "\n",
        "    Returns:\n",
        "      (tables, table_id_map)\n",
        "        - tables: merged table list\n",
        "        - table_id_map: maps original table_{i:04d} -> merged/semantic table_id\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------------\n",
        "    # Page geometry stats (coord-system agnostic)\n",
        "    # ----------------------------\n",
        "    def page_minmax_y():\n",
        "        mm = defaultdict(lambda: [None, None])  # page -> [ymin, ymax]\n",
        "        for t in doc.get(\"texts\", []):\n",
        "            prov0 = (t.get(\"prov\") or [{}])[0]\n",
        "            p = prov0.get(\"page_no\")\n",
        "            bb = prov0.get(\"bbox\") or {}\n",
        "            y = bb.get(\"t\") if isinstance(bb, dict) else None\n",
        "            if p is None or y is None:\n",
        "                continue\n",
        "            lo, hi = mm[p]\n",
        "            mm[p][0] = y if lo is None else min(lo, y)\n",
        "            mm[p][1] = y if hi is None else max(hi, y)\n",
        "        return {k: (v[0], v[1]) for k, v in mm.items() if v[0] is not None and v[1] is not None and v[1] > v[0]}\n",
        "\n",
        "    mm = page_minmax_y()\n",
        "    print(f\"DEBUG build_tables_json: mm pages = {sorted(mm.keys())}\")\n",
        "\n",
        "    def y_norm(page_no, y):\n",
        "        if page_no not in mm or y is None:\n",
        "            return None\n",
        "        lo, hi = mm[page_no]\n",
        "        return (y - lo) / (hi - lo) if hi > lo else None\n",
        "\n",
        "    def infer_top_is_high_by_page():\n",
        "        \"\"\"\n",
        "        Decide whether the physical page-top corresponds to high y values (True) or low y values (False).\n",
        "        Heuristic: whichever extreme band has longer average text is treated as the header side.\n",
        "        \"\"\"\n",
        "        out = {}\n",
        "        for p in mm.keys():\n",
        "            hi_band = []\n",
        "            lo_band = []\n",
        "            for obj in doc.get(\"texts\", []):\n",
        "                prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "                if prov0.get(\"page_no\") != p:\n",
        "                    continue\n",
        "                bb = prov0.get(\"bbox\") or {}\n",
        "                if not isinstance(bb, dict):\n",
        "                    continue\n",
        "                tt = bb.get(\"t\"); bbv = bb.get(\"b\")\n",
        "                if tt is None or bbv is None:\n",
        "                    continue\n",
        "                ymid = (tt + bbv) / 2.0\n",
        "                yn = y_norm(p, ymid)\n",
        "                if yn is None:\n",
        "                    continue\n",
        "                txt = normalize_ws(obj.get(\"text\") or \"\")\n",
        "                if not txt:\n",
        "                    continue\n",
        "                if yn >= 0.90:\n",
        "                    hi_band.append(len(txt))\n",
        "                elif yn <= 0.10:\n",
        "                    lo_band.append(len(txt))\n",
        "\n",
        "            if hi_band and lo_band:\n",
        "                out[p] = (sum(hi_band) / len(hi_band)) >= (sum(lo_band) / len(lo_band))\n",
        "            elif hi_band and not lo_band:\n",
        "                out[p] = True\n",
        "            elif lo_band and not hi_band:\n",
        "                out[p] = False\n",
        "            else:\n",
        "                out[p] = True\n",
        "        return out\n",
        "\n",
        "    top_is_high = infer_top_is_high_by_page()\n",
        "\n",
        "    def phys_topness(page_no, y):\n",
        "        \"\"\"\n",
        "        Convert y into a 'physical topness' score in [0,1],\n",
        "        where 1 means physically near the top of the page.\n",
        "        \"\"\"\n",
        "        yn = y_norm(page_no, y)\n",
        "        if yn is None:\n",
        "            return None\n",
        "        return yn if top_is_high.get(page_no, True) else (1.0 - yn)\n",
        "\n",
        "    def x_overlap_ratio(a_l, a_r, b_l, b_r):\n",
        "        if None in (a_l, a_r, b_l, b_r):\n",
        "            return 0.0\n",
        "        inter = max(0.0, min(a_r, b_r) - max(a_l, b_l))\n",
        "        aw = max(1e-6, a_r - a_l)\n",
        "        bw = max(1e-6, b_r - b_l)\n",
        "        return inter / min(aw, bw)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Caption parsing + inference\n",
        "    # ----------------------------\n",
        "    TABLE_NUM_RE = re.compile(r\"(?i)\\b(?:table|tbl\\.?)\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
        "\n",
        "    def parse_table_number(caption: str):\n",
        "        if not caption:\n",
        "            return None\n",
        "        m = TABLE_NUM_RE.search(caption)\n",
        "        return m.group(1) if m else None\n",
        "\n",
        "    def sanitize_caption(caption: str, rows) -> str:\n",
        "      caption = normalize_ws(caption or \"\")\n",
        "      if not caption:\n",
        "          return \"\"\n",
        "\n",
        "      # IMPORTANT: never discard a strong \"Table X.Y:\" caption even if it matches row[0]\n",
        "      if TABLE_CAPTION_STRONG_RE.match(caption):\n",
        "          return caption\n",
        "\n",
        "      # Otherwise, prevent using column headers as captions\n",
        "      if rows:\n",
        "          hdr = normalize_ws(\" \".join([c for c in (rows[0] or []) if c]))\n",
        "          if hdr and canonicalize_repeat_text(caption) == canonicalize_repeat_text(hdr):\n",
        "              return \"\"\n",
        "      return caption\n",
        "\n",
        "\n",
        "    def infer_caption_from_first_row(rows):\n",
        "        \"\"\"\n",
        "        If Docling swallowed the caption into the first table row, recover it.\n",
        "        Prefer a single cell that matches \"Table <no>: ...\" over concatenating the whole row.\n",
        "        \"\"\"\n",
        "        if not rows or not rows[0]:\n",
        "            return \"\"\n",
        "\n",
        "        cells = [normalize_ws(c) for c in rows[0] if normalize_ws(c)]\n",
        "\n",
        "        # 1) If any single cell is a strong caption, use that (best behavior for your case)\n",
        "        for c in cells:\n",
        "            if TABLE_CAPTION_STRONG_RE.match(c):\n",
        "                return c\n",
        "\n",
        "        # 2) Otherwise, try the concatenated row\n",
        "        row0_joined = normalize_ws(\" \".join(cells))\n",
        "        if TABLE_CAPTION_STRONG_RE.match(row0_joined):\n",
        "            return row0_joined\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "    def infer_table_caption(page_no, table_bbox, rows):\n",
        "        \"\"\"\n",
        "        Find a likely caption line near the table (usually above it),\n",
        "        using layout + proximity, without hardcoding language.\n",
        "        \"\"\"\n",
        "        if page_no is None or not isinstance(table_bbox, dict) or page_no not in mm:\n",
        "            print(f\"DEBUG: Early return - page_no={page_no}, in_mm={page_no in mm if page_no else 'N/A'}\")\n",
        "            return \"\"\n",
        "\n",
        "        t_l = table_bbox.get(\"l\"); t_r = table_bbox.get(\"r\")\n",
        "        t_t = table_bbox.get(\"t\"); t_b = table_bbox.get(\"b\")\n",
        "        if None in (t_l, t_r, t_t, t_b):\n",
        "            print(f\"DEBUG: Early return - bbox incomplete\")\n",
        "            return \"\"\n",
        "\n",
        "        # Determine table physical top boundary (y coordinate)\n",
        "        table_low, table_high = (t_t, t_b) if t_t < t_b else (t_b, t_t)\n",
        "        span = mm[page_no][1] - mm[page_no][0]\n",
        "\n",
        "        # Use coord_origin directly from bbox instead of flawed heuristic\n",
        "        coord_origin = (table_bbox.get(\"coord_origin\") or \"\").upper()\n",
        "        # In BOTTOMLEFT, high y = physical top; in TOPLEFT, low y = physical top\n",
        "        top_high = (coord_origin == \"BOTTOMLEFT\")\n",
        "\n",
        "        table_phys_top = table_high if top_high else table_low\n",
        "\n",
        "        print(f\"DEBUG: page={page_no}, table_bbox={table_bbox}\")\n",
        "        print(f\"DEBUG: span={span}, coord_origin={coord_origin}, top_high={top_high}, table_phys_top={table_phys_top}\")\n",
        "\n",
        "        header_row_text = \"\"\n",
        "        if rows:\n",
        "            header_row_text = normalize_ws(\" \".join([c for c in (rows[0] or []) if c]))\n",
        "\n",
        "        best = None  # (score, raw, obj)\n",
        "        for max_gap_frac in (0.08, 0.22):\n",
        "            max_gap = max_gap_frac * span\n",
        "\n",
        "            for obj in doc.get(\"texts\", []):\n",
        "                prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "                if prov0.get(\"page_no\") != page_no:\n",
        "                    continue\n",
        "                bb = prov0.get(\"bbox\") or {}\n",
        "                if not isinstance(bb, dict):\n",
        "                    continue\n",
        "                l = bb.get(\"l\"); r = bb.get(\"r\")\n",
        "                tt = bb.get(\"t\"); bbv = bb.get(\"b\")\n",
        "                if None in (l, r, tt, bbv):\n",
        "                    continue\n",
        "\n",
        "                raw = normalize_ws(obj.get(\"text\") or \"\")\n",
        "                if not raw or is_noise_text(raw):\n",
        "                    continue\n",
        "                strong = bool(TABLE_CAPTION_STRONG_RE.match(raw))\n",
        "                max_len = 260 if strong else 180\n",
        "                if len(raw) > max_len:\n",
        "                    continue\n",
        "\n",
        "                x_overlap = x_overlap_ratio(l, r, t_l, t_r)\n",
        "                x_min = 0.10 if strong else 0.30\n",
        "                if x_overlap < x_min:\n",
        "                    continue\n",
        "\n",
        "                text_low, text_high = (tt, bbv) if tt < bbv else (bbv, tt)\n",
        "\n",
        "                # Gap: caption is just above table physical top\n",
        "                # For BOTTOMLEFT (top_high=True): caption_bottom (text_low) should be just above table_top (table_high)\n",
        "                #   gap = caption_bottom - table_top = text_low - table_phys_top (should be small positive when caption is just above)\n",
        "                # For TOPLEFT (top_high=False): caption_bottom (text_high) should be just above table_top (table_low)\n",
        "                #   gap = table_top - caption_bottom = table_phys_top - text_high (should be small positive when caption is just above)\n",
        "                tol = (0.03 if strong else 0.015) * span\n",
        "                if top_high:\n",
        "                    # BOTTOMLEFT: caption's low y should be >= table's high y (caption is above)\n",
        "                    gap = text_low - table_phys_top\n",
        "                else:\n",
        "                    # TOPLEFT: caption's high y should be <= table's low y (caption is above)\n",
        "                    gap = table_phys_top - text_high\n",
        "\n",
        "                # Debug: Show candidates that start with \"Table\"\n",
        "                if \"Table\" in raw or \"table\" in raw.lower():\n",
        "                    print(f\"DEBUG CANDIDATE: '{raw[:80]}...'\")\n",
        "                    print(f\"  text_bbox: l={l}, t={tt}, r={r}, b={bbv}\")\n",
        "                    print(f\"  text_low={text_low}, text_high={text_high}\")\n",
        "                    print(f\"  x_overlap={x_overlap:.3f}, gap={gap:.3f}, tol={tol:.3f}, max_gap={max_gap:.3f}\")\n",
        "                    print(f\"  gap_check: gap < -tol = {gap < -tol}, gap > max_gap = {gap > max_gap}\")\n",
        "\n",
        "                eff_max_gap = max(max_gap, 0.35 * span) if strong else max_gap\n",
        "                neg_tol = (0.06 * span) if strong else tol   # allow overlap for \"Table X: ...\"\n",
        "                if gap < -neg_tol or gap > eff_max_gap:\n",
        "                    continue\n",
        "                gap = max(0.0, gap)\n",
        "\n",
        "\n",
        "                # Avoid picking the header row itself if it appears as separate text\n",
        "                if header_row_text and canonicalize_repeat_text(raw) == canonicalize_repeat_text(header_row_text):\n",
        "                    continue\n",
        "\n",
        "                # Score: closer is better; mild bonus for digits\n",
        "                score = - (gap / max(1e-6, span))\n",
        "                if strong:\n",
        "                    score += 0.10\n",
        "                if any(unicodedata.category(ch) == \"Nd\" for ch in raw):\n",
        "                    score += 0.02\n",
        "                if len(raw) <= 90:\n",
        "                    score += 0.01\n",
        "\n",
        "                if best is None or score > best[0]:\n",
        "                    best = (score, raw, obj)\n",
        "\n",
        "            if best is not None:\n",
        "                break\n",
        "\n",
        "        if best:\n",
        "            k = _make_text_key_from_obj(best[2])\n",
        "            if k is not None:\n",
        "                CAPTION_TEXT_KEYS.add(k)\n",
        "            return best[1]\n",
        "        return \"\"\n",
        "\n",
        "    # ----------------------------\n",
        "    # Build raw tables\n",
        "    # ----------------------------\n",
        "    raw_tables = []\n",
        "    for i, tbl in enumerate(doc.get(\"tables\", [])):\n",
        "        provs = tbl.get(\"prov\") or []\n",
        "        page_no = provs[0].get(\"page_no\") if provs else None\n",
        "        bbox = provs[0].get(\"bbox\") if provs else None\n",
        "\n",
        "        t = table_rows_robust(tbl)\n",
        "        rows = t.get(\"rows\") or []\n",
        "\n",
        "        caption = extract_caption(doc, tbl.get(\"captions\"))\n",
        "        if not caption:\n",
        "            caption = infer_table_caption(page_no, bbox or {}, rows)\n",
        "        if not caption:\n",
        "            caption = infer_caption_from_first_row(rows)\n",
        "\n",
        "        caption = sanitize_caption(caption, rows)\n",
        "\n",
        "        raw_id = f\"table_{i:04d}\"\n",
        "        raw_tables.append({\n",
        "            \"table_id\": raw_id,  # temporary; will be remapped later\n",
        "            \"raw_table_id\": raw_id,\n",
        "            \"page_no\": page_no,\n",
        "            \"bbox\": bbox,\n",
        "            \"caption\": caption,\n",
        "            \"num_rows\": t.get(\"num_rows\"),\n",
        "            \"num_cols\": t.get(\"num_cols\"),\n",
        "            \"rows\": rows,\n",
        "        })\n",
        "\n",
        "    # Sort by page then physical-topness (top-to-bottom reading order)\n",
        "    def tbl_key(t):\n",
        "        p = t.get(\"page_no\") or 10**9\n",
        "        bb = t.get(\"bbox\") or {}\n",
        "        tt = bb.get(\"t\")\n",
        "        topn = phys_topness(p, tt) if tt is not None else None\n",
        "        # Higher physical topness first, so reading order top->bottom\n",
        "        return (p, -(topn if topn is not None else -1))\n",
        "\n",
        "    raw_tables = sorted(raw_tables, key=tbl_key)\n",
        "\n",
        "    def header_candidates(table, max_rows=3):\n",
        "      \"\"\"\n",
        "      Return canonicalized header-like strings from the first few rows.\n",
        "      We use the full row text (not just [0][0]) because Docling often splits headers.\n",
        "      \"\"\"\n",
        "      rows = table.get(\"rows\") or []\n",
        "      cands = []\n",
        "      for i in range(min(max_rows, len(rows))):\n",
        "          row_txt = \" \".join([c for c in (rows[i] or []) if normalize_ws(c)])\n",
        "          row_txt = canonicalize_repeat_text(row_txt)\n",
        "          if row_txt and len(row_txt) >= 6:\n",
        "              cands.append(row_txt)\n",
        "      return cands\n",
        "\n",
        "\n",
        "    def header_similarity(prev, cur):\n",
        "        a = header_candidates(prev)\n",
        "        b = header_candidates(cur)\n",
        "        if not a or not b:\n",
        "            return 0.0\n",
        "        best = 0.0\n",
        "        for x in a:\n",
        "            for y in b:\n",
        "                best = max(best, SequenceMatcher(None, x, y).ratio())\n",
        "        return best\n",
        "\n",
        "\n",
        "    def table_x_overlap(prev, cur):\n",
        "        pb = prev.get(\"bbox\") or {}\n",
        "        cb = cur.get(\"bbox\") or {}\n",
        "        return x_overlap_ratio(pb.get(\"l\"), pb.get(\"r\"), cb.get(\"l\"), cb.get(\"r\"))\n",
        "\n",
        "\n",
        "    def should_merge(prev, cur):\n",
        "        # Basic adjacency\n",
        "        if prev.get(\"page_no\") is None or cur.get(\"page_no\") is None:\n",
        "            return False\n",
        "        if cur[\"page_no\"] != prev[\"page_no\"] + 1:\n",
        "            return False\n",
        "        if cur.get(\"num_cols\") != prev.get(\"num_cols\"):\n",
        "            return False\n",
        "\n",
        "        # Continuation cues from captions/table numbers (strongest)\n",
        "        prev_cap = (prev.get(\"caption\") or \"\").strip()\n",
        "        cur_cap  = (cur.get(\"caption\")  or \"\").strip()\n",
        "        prev_no = table_no_from_caption(prev_cap)\n",
        "        cur_no  = table_no_from_caption(cur_cap)\n",
        "\n",
        "        # Geometry overlap sanity (prevents accidental merges of unrelated tables)\n",
        "        xo = table_x_overlap(prev, cur)\n",
        "        if xo < 0.55:  # tune 0.45–0.70 depending on your docs\n",
        "            return False\n",
        "\n",
        "        # If both have numbers and they match -> merge even if headers differ\n",
        "        if prev_no and cur_no and prev_no == cur_no:\n",
        "            return True\n",
        "\n",
        "        # If prev has a number and current has no caption/number -> likely continuation\n",
        "        if prev_no and not cur_no and not cur_cap:\n",
        "            return True\n",
        "\n",
        "        # If prev has any caption and current has none -> likely continuation\n",
        "        if prev_cap and not cur_cap:\n",
        "            return True\n",
        "\n",
        "        # Otherwise, allow fuzzy header match (instead of exact)\n",
        "        sim = header_similarity(prev, cur)\n",
        "        if sim >= 0.78:  # tune 0.70–0.85\n",
        "            return True\n",
        "\n",
        "        # Final fallback: your spatial “bottom of prev / top of cur” rule\n",
        "        pb = (prev.get(\"bbox\") or {}).get(\"b\")\n",
        "        ct = (cur.get(\"bbox\") or {}).get(\"t\")\n",
        "        if pb is None or ct is None:\n",
        "            return False\n",
        "\n",
        "        pt = phys_topness(prev[\"page_no\"], pb)\n",
        "        tt = phys_topness(cur[\"page_no\"], ct)\n",
        "        if pt is None or tt is None:\n",
        "            return False\n",
        "\n",
        "        prev_bottomness = 1.0 - pt\n",
        "        cur_topness = tt\n",
        "        thresh = 0.80\n",
        "        return (prev_bottomness >= thresh) or (cur_topness >= thresh)\n",
        "\n",
        "    _TABLE_NO_RE = re.compile(r\"(?i)\\btable\\b\\.?\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
        "\n",
        "    def table_no_from_caption(caption: str):\n",
        "        if not caption:\n",
        "            return None\n",
        "        m = _TABLE_NO_RE.search(caption)\n",
        "        return m.group(1) if m else None\n",
        "\n",
        "\n",
        "    def rows_equal(a, b):\n",
        "        return canonicalize_repeat_text(\" \".join(a or [])) == canonicalize_repeat_text(\" \".join(b or []))\n",
        "\n",
        "    # ----------------------------\n",
        "    # Merge continuation fragments\n",
        "    # ----------------------------\n",
        "    merged = []\n",
        "    table_id_map = {}\n",
        "\n",
        "    for cur in raw_tables:\n",
        "        if not merged:\n",
        "            merged.append(cur)\n",
        "            table_id_map[cur[\"raw_table_id\"]] = cur[\"table_id\"]\n",
        "            continue\n",
        "\n",
        "        prev = merged[-1]\n",
        "        if should_merge(prev, cur):\n",
        "            # Drop duplicated header row if present\n",
        "            cur_rows = cur.get(\"rows\") or []\n",
        "            if cur_rows and (prev.get(\"rows\") or []) and rows_equal(cur_rows[0], prev[\"rows\"][0]):\n",
        "                cur_rows = cur_rows[1:]\n",
        "\n",
        "            prev[\"rows\"].extend(cur_rows)\n",
        "            prev[\"num_rows\"] = len(prev[\"rows\"])\n",
        "            prev.setdefault(\"page_span\", [prev.get(\"page_no\"), prev.get(\"page_no\")])\n",
        "            prev[\"page_span\"][1] = cur.get(\"page_no\")\n",
        "\n",
        "            # Keep earliest non-empty caption\n",
        "            if not prev.get(\"caption\") and cur.get(\"caption\"):\n",
        "                prev[\"caption\"] = cur[\"caption\"]\n",
        "\n",
        "            table_id_map[cur[\"raw_table_id\"]] = prev[\"table_id\"]\n",
        "        else:\n",
        "            merged.append(cur)\n",
        "            table_id_map[cur[\"raw_table_id\"]] = cur[\"table_id\"]\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rename tables to semantic IDs from captions\n",
        "    # ----------------------------\n",
        "    final_id_map = {}\n",
        "    used = set()\n",
        "    for t in merged:\n",
        "        old_id = t.get(\"table_id\")\n",
        "        num = parse_table_number(t.get(\"caption\", \"\"))\n",
        "        if num:\n",
        "            new_id = num\n",
        "            if new_id in used:\n",
        "                new_id = f\"{new_id}_p{t.get('page_no')}\"\n",
        "            used.add(new_id)\n",
        "        else:\n",
        "            new_id = old_id\n",
        "\n",
        "        t[\"table_id\"] = new_id\n",
        "        final_id_map[old_id] = new_id\n",
        "\n",
        "    # Update mapping for raw ids -> final ids\n",
        "    table_id_map = {raw: final_id_map.get(mid, mid) for raw, mid in table_id_map.items()}\n",
        "\n",
        "    return merged, table_id_map\n",
        "\n",
        "\n",
        "def build_figures_json(doc: dict):\n",
        "    \"\"\"\n",
        "    Build figure JSON with:\n",
        "      1) caption extraction/inference from nearby text\n",
        "      2) semantic figure_id derived from caption number (e.g., \"6.1.1\")\n",
        "\n",
        "    Returns:\n",
        "      (figures, figure_id_map)\n",
        "        - figures: list of figure records (no file yet)\n",
        "        - figure_id_map: maps raw figure_{idx:04d} -> final figure_id\n",
        "    \"\"\"\n",
        "\n",
        "    def page_minmax_y():\n",
        "        mm = defaultdict(lambda: [None, None])\n",
        "        for t in doc.get(\"texts\", []):\n",
        "            prov0 = (t.get(\"prov\") or [{}])[0]\n",
        "            p = prov0.get(\"page_no\")\n",
        "            bb = prov0.get(\"bbox\") or {}\n",
        "            y = bb.get(\"t\") if isinstance(bb, dict) else None\n",
        "            if p is None or y is None:\n",
        "                continue\n",
        "            lo, hi = mm[p]\n",
        "            mm[p][0] = y if lo is None else min(lo, y)\n",
        "            mm[p][1] = y if hi is None else max(hi, y)\n",
        "        return {k: (v[0], v[1]) for k, v in mm.items() if v[0] is not None and v[1] is not None and v[1] > v[0]}\n",
        "\n",
        "    mm = page_minmax_y()\n",
        "\n",
        "    def y_norm(page_no, y):\n",
        "        if page_no not in mm or y is None:\n",
        "            return None\n",
        "        lo, hi = mm[page_no]\n",
        "        return (y - lo) / (hi - lo) if hi > lo else None\n",
        "\n",
        "    def infer_top_is_high_by_page():\n",
        "        out = {}\n",
        "        for p in mm.keys():\n",
        "            hi_band, lo_band = [], []\n",
        "            for obj in doc.get(\"texts\", []):\n",
        "                prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "                if prov0.get(\"page_no\") != p:\n",
        "                    continue\n",
        "                bb = prov0.get(\"bbox\") or {}\n",
        "                if not isinstance(bb, dict):\n",
        "                    continue\n",
        "                tt = bb.get(\"t\"); bbv = bb.get(\"b\")\n",
        "                if tt is None or bbv is None:\n",
        "                    continue\n",
        "                ymid = (tt + bbv) / 2.0\n",
        "                yn = y_norm(p, ymid)\n",
        "                if yn is None:\n",
        "                    continue\n",
        "                txt = normalize_ws(obj.get(\"text\") or \"\")\n",
        "                if not txt:\n",
        "                    continue\n",
        "                if yn >= 0.90:\n",
        "                    hi_band.append(len(txt))\n",
        "                elif yn <= 0.10:\n",
        "                    lo_band.append(len(txt))\n",
        "            if hi_band and lo_band:\n",
        "                out[p] = (sum(hi_band) / len(hi_band)) >= (sum(lo_band) / len(lo_band))\n",
        "            elif hi_band and not lo_band:\n",
        "                out[p] = True\n",
        "            elif lo_band and not hi_band:\n",
        "                out[p] = False\n",
        "            else:\n",
        "                out[p] = True\n",
        "        return out\n",
        "\n",
        "    top_is_high = infer_top_is_high_by_page()\n",
        "\n",
        "    def x_overlap_ratio(a_l, a_r, b_l, b_r):\n",
        "        if None in (a_l, a_r, b_l, b_r):\n",
        "            return 0.0\n",
        "        inter = max(0.0, min(a_r, b_r) - max(a_l, b_l))\n",
        "        aw = max(1e-6, a_r - a_l)\n",
        "        bw = max(1e-6, b_r - b_l)\n",
        "        return inter / min(aw, bw)\n",
        "\n",
        "    FIGURE_NUM_RE = re.compile(r\"(?i)\\b(?:figure|fig\\.?)\\s*([0-9]+(?:\\.[0-9]+)*)\")\n",
        "\n",
        "    def parse_figure_number(caption: str):\n",
        "        if not caption:\n",
        "            return None\n",
        "        m = FIGURE_NUM_RE.search(caption)\n",
        "        return m.group(1) if m else None\n",
        "\n",
        "    FIGURE_CAPTION_STRONG_RE = re.compile(\n",
        "    r\"^\\s*(?:Figure|Fig\\.?)\\s+\\d+(?:\\.\\d+)*\\s*[:\\-–]?\\s*\\S\",\n",
        "    re.IGNORECASE\n",
        "    )\n",
        "\n",
        "\n",
        "    def infer_figure_caption(page_no, fig_bbox):\n",
        "      if page_no is None or not isinstance(fig_bbox, dict) or page_no not in mm:\n",
        "          return \"\"\n",
        "\n",
        "      f_l = fig_bbox.get(\"l\"); f_r = fig_bbox.get(\"r\")\n",
        "      f_t = fig_bbox.get(\"t\"); f_b = fig_bbox.get(\"b\")\n",
        "      if None in (f_l, f_r, f_t, f_b):\n",
        "          return \"\"\n",
        "\n",
        "      # Use coord_origin directly (more reliable than heuristics)\n",
        "      coord_origin = (fig_bbox.get(\"coord_origin\") or \"\").upper()\n",
        "      top_high = (coord_origin == \"BOTTOMLEFT\")  # high y is physical top in BOTTOMLEFT\n",
        "\n",
        "      fig_low, fig_high = (f_t, f_b) if f_t < f_b else (f_b, f_t)\n",
        "      span = mm[page_no][1] - mm[page_no][0]\n",
        "\n",
        "      fig_phys_top = fig_high if top_high else fig_low\n",
        "      fig_phys_bot = fig_low if top_high else fig_high\n",
        "\n",
        "      best = None  # (score, raw, obj)\n",
        "\n",
        "      # Search further because captions can be separated by notes\n",
        "      for max_gap_frac in (0.25, 0.55):\n",
        "          max_gap = max_gap_frac * span\n",
        "\n",
        "          for obj in doc.get(\"texts\", []):\n",
        "              prov0 = (obj.get(\"prov\") or [{}])[0]\n",
        "              if prov0.get(\"page_no\") != page_no:\n",
        "                  continue\n",
        "              bb = prov0.get(\"bbox\") or {}\n",
        "              if not isinstance(bb, dict):\n",
        "                  continue\n",
        "\n",
        "              l = bb.get(\"l\"); r = bb.get(\"r\")\n",
        "              tt = bb.get(\"t\"); bbv = bb.get(\"b\")\n",
        "              if None in (l, r, tt, bbv):\n",
        "                  continue\n",
        "\n",
        "              raw = normalize_ws(obj.get(\"text\") or \"\")\n",
        "              if not raw or is_noise_text(raw):\n",
        "                  continue\n",
        "\n",
        "              strong = bool(FIGURE_CAPTION_STRONG_RE.match(raw))\n",
        "              max_len = 320 if strong else 180\n",
        "              if len(raw) > max_len:\n",
        "                  continue\n",
        "\n",
        "              # X overlap: relax for strong captions\n",
        "              if x_overlap_ratio(l, r, f_l, f_r) < (0.10 if strong else 0.25):\n",
        "                  continue\n",
        "\n",
        "              text_low, text_high = (tt, bbv) if tt < bbv else (bbv, tt)\n",
        "\n",
        "              # gaps above/below (physical)\n",
        "              gap_above = (text_low - fig_phys_top) if top_high else (fig_phys_top - text_high)\n",
        "              gap_below = (fig_phys_bot - text_high) if top_high else (text_low - fig_phys_bot)\n",
        "\n",
        "              # Prefer below-caption first; allow far gaps only if it's a strong \"Figure X\" line\n",
        "              candidates = []\n",
        "              tol = (0.06 if strong else 0.02) * span\n",
        "              eff_max_gap = max_gap if not strong else max(max_gap, 0.70 * span)\n",
        "\n",
        "              if -tol <= gap_below <= eff_max_gap:\n",
        "                  candidates.append(max(0.0, gap_below))\n",
        "              if -tol <= gap_above <= eff_max_gap:\n",
        "                  candidates.append(max(0.0, gap_above))\n",
        "\n",
        "              if not candidates:\n",
        "                  continue\n",
        "\n",
        "              gap = min(candidates)\n",
        "\n",
        "              score = -(gap / max(1e-6, span))\n",
        "              if strong:\n",
        "                  score += 0.15\n",
        "              if raw.lower().startswith((\"figure\", \"fig.\")):\n",
        "                  score += 0.05\n",
        "\n",
        "              if best is None or score > best[0]:\n",
        "                  best = (score, raw, obj)\n",
        "\n",
        "          if best is not None:\n",
        "              break\n",
        "\n",
        "      if best:\n",
        "          k = _make_text_key_from_obj(best[2])\n",
        "          if k is not None:\n",
        "              CAPTION_TEXT_KEYS.add(k)\n",
        "          return best[1]\n",
        "\n",
        "      return \"\"\n",
        "\n",
        "    figures = []\n",
        "    figure_id_map = {}\n",
        "    used = set()\n",
        "\n",
        "    for i, pic in enumerate(doc.get(\"pictures\", [])):\n",
        "        provs = pic.get(\"prov\") or []\n",
        "        page_no = provs[0].get(\"page_no\") if provs else None\n",
        "        bbox = provs[0].get(\"bbox\") if provs else None\n",
        "\n",
        "        caption = extract_caption(doc, pic.get(\"captions\"))\n",
        "        if not caption:\n",
        "            caption = infer_figure_caption(page_no, bbox or {})\n",
        "        caption = normalize_ws(caption)\n",
        "\n",
        "        raw_id = f\"figure_{i:04d}\"\n",
        "        num = parse_figure_number(caption)\n",
        "        if num:\n",
        "            new_id = num\n",
        "            if new_id in used:\n",
        "                new_id = f\"{new_id}_p{page_no}\"\n",
        "            used.add(new_id)\n",
        "        else:\n",
        "            new_id = raw_id\n",
        "\n",
        "        figure_id_map[raw_id] = new_id\n",
        "        figures.append({\n",
        "            \"figure_id\": new_id,\n",
        "            \"raw_figure_id\": raw_id,\n",
        "            \"page_no\": page_no,\n",
        "            \"bbox\": bbox,\n",
        "            \"caption\": caption,\n",
        "        })\n",
        "\n",
        "    return figures, figure_id_map\n",
        "\n",
        "\n",
        "def bbox_to_fitz_rect(bbox: dict, page_height: float):\n",
        "    \"\"\"\n",
        "    Convert Docling bbox to a PyMuPDF Rect.\n",
        "    Docling bbox is usually BOTTOMLEFT. PyMuPDF uses TOPLEFT origin.\n",
        "    \"\"\"\n",
        "    if not bbox:\n",
        "        return None\n",
        "    x0, y0, x1, y1 = bbox.get(\"l\"), bbox.get(\"t\"), bbox.get(\"r\"), bbox.get(\"b\")\n",
        "    if None in (x0, y0, x1, y1):\n",
        "        return None\n",
        "    # Convert y if bbox origin is bottom-left:\n",
        "    # y_top = page_height - y_max, y_bottom = page_height - y_min\n",
        "    origin = (bbox.get(\"coord_origin\") or \"\").upper()\n",
        "    if origin == \"BOTTOMLEFT\":\n",
        "        y0, y1 = page_height - y0, page_height - y1\n",
        "    y0, y1 = sorted([y0, y1])\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "def extract_figures_from_pdf(figures: list, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    \"\"\"Render and save figure crops given figure records with bbox/page metadata.\"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    images_meta = []\n",
        "\n",
        "    for fig in figures:\n",
        "        page_no = fig.get(\"page_no\")\n",
        "        bbox = fig.get(\"bbox\")\n",
        "        figure_id = fig.get(\"figure_id\")\n",
        "        if not page_no or not bbox or not figure_id:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page.rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "        clip = clip & page.rect\n",
        "\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        safe_id = str(figure_id).replace(\".\", \"_\")\n",
        "        fname = f\"figure_{safe_id}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        images_meta.append(\n",
        "            {\n",
        "                \"figure_id\": figure_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": fig.get(\"caption\", \"\"),\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return images_meta\n",
        "\n",
        "# ----------------------------\n",
        "# Equations (FORMULA items)\n",
        "# ----------------------------\n",
        "def is_formula_text_item(text_obj: dict) -> bool:\n",
        "    return (text_obj.get(\"label\") or \"\").strip().upper() == \"FORMULA\"\n",
        "\n",
        "\n",
        "def maybe_despace_latex(latex: str) -> str:\n",
        "    s = latex.strip()\n",
        "    toks = s.split()\n",
        "    if len(toks) < 8:\n",
        "        return s\n",
        "    single = sum(1 for t in toks if len(t) == 1)\n",
        "    if single / max(1, len(toks)) >= 0.6:\n",
        "        return \"\".join(toks)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_formula_latex(text_obj: dict) -> str:\n",
        "    raw = text_obj.get(\"latex\") or text_obj.get(\"text\") or \"\"\n",
        "    raw = normalize_ws(raw.strip())\n",
        "    return maybe_despace_latex(raw) if FIX_SPACED_LATEX else raw\n",
        "\n",
        "\n",
        "def extract_equations_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    equations_meta = []\n",
        "\n",
        "    for idx, t in enumerate(doc.get(\"texts\", []) or []):\n",
        "        if not is_formula_text_item(t):\n",
        "            continue\n",
        "\n",
        "        provs = t.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page.rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page.rect\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        eq_id = f\"eq_{idx:05d}\"\n",
        "        fname = f\"{eq_id}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        equations_meta.append(\n",
        "            {\n",
        "                \"equation_id\": eq_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"latex\": extract_formula_latex(t),\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return equations_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Clause tree\n",
        "# ----------------------------\n",
        "def build_clause_tree(doc: dict, table_id_map=None, table_caption_map=None, figure_id_map=None, figure_caption_map=None):\n",
        "    \"\"\"\n",
        "    Builds nodes keyed by clause id.\n",
        "    Stores:\n",
        "      - text: paragraphs (excluding list items)\n",
        "      - lists: nested list blocks\n",
        "      - equations: refs + latex\n",
        "      - figures: captions only\n",
        "      - tables: refs + captions\n",
        "    \"\"\"\n",
        "    nodes = {}\n",
        "    root_id = \"ROOT\"\n",
        "    nodes[root_id] = {\n",
        "        \"id\": root_id,\n",
        "        \"title\": \"\",\n",
        "        \"children\": [],\n",
        "        \"tables\": [],\n",
        "        \"figures\": [],\n",
        "        \"equations\": [],\n",
        "        \"text\": \"\",\n",
        "        \"lists\": [],\n",
        "        \"_blocks\": [],\n",
        "    }\n",
        "    current_id = root_id\n",
        "    table_id_map = table_id_map or {}\n",
        "    table_caption_map = table_caption_map or {}\n",
        "    figure_id_map = figure_id_map or {}\n",
        "    figure_caption_map = figure_caption_map or {}\n",
        "\n",
        "\n",
        "    def ensure_node(cid: str):\n",
        "        if cid not in nodes:\n",
        "            nodes[cid] = {\n",
        "                \"id\": cid,\n",
        "                \"title\": \"\",\n",
        "                \"children\": [],\n",
        "                \"tables\": [],\n",
        "                \"figures\": [],\n",
        "                \"equations\": [],\n",
        "                \"text\": \"\",\n",
        "                \"lists\": [],\n",
        "                \"_blocks\": [],\n",
        "            }\n",
        "\n",
        "    def parent_id(cid: str) -> str:\n",
        "        parts = cid.split(\".\")\n",
        "        return root_id if len(parts) <= 1 else \".\".join(parts[:-1])\n",
        "\n",
        "    def add_child(pid: str, cid: str):\n",
        "        if cid not in nodes[pid][\"children\"]:\n",
        "            nodes[pid][\"children\"].append(cid)\n",
        "\n",
        "    def add_text_block(cid: str, txt: str):\n",
        "        txt = (txt or \"\").rstrip()\n",
        "        if not txt:\n",
        "            return\n",
        "        nodes[cid][\"_blocks\"].append({\"kind\": \"text\", \"text\": txt})\n",
        "\n",
        "    def add_list_block(cid: str, marker: str, txt: str, indent: float, page_no=None, bbox=None):\n",
        "        nodes[cid][\"_blocks\"].append(\n",
        "            {\n",
        "                \"kind\": \"list_item\",\n",
        "                \"marker\": marker,\n",
        "                \"text\": txt,\n",
        "                \"indent\": indent,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    for ref in walk_body_in_reading_order(doc):\n",
        "        kind, idx, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"texts\":\n",
        "            label = (obj.get(\"label\") or \"\").strip().lower()\n",
        "\n",
        "            # --- STRATEGY: ENHANCED LATEX EXTRACTION ---\n",
        "            # Prioritize the 'latex' field if it exists, as it contains\n",
        "            # the conversion of GLYPH symbols.\n",
        "            latex_val = obj.get(\"latex\")\n",
        "            if latex_val:\n",
        "                raw = f\"$${extract_formula_latex(obj)}$$\"\n",
        "            else:\n",
        "                raw = obj.get(\"text\") or \"\"\n",
        "            # -------------------------------------------\n",
        "\n",
        "            if is_caption_text_obj(obj):\n",
        "                continue\n",
        "            if not raw.strip():\n",
        "                continue\n",
        "\n",
        "            provs = obj.get(\"prov\") or []\n",
        "            prov0 = provs[0] if provs else {}\n",
        "            page_no = prov0.get(\"page_no\")\n",
        "            bbox = prov0.get(\"bbox\")\n",
        "            geo_indent = float(bbox.get(\"l\")) if isinstance(bbox, dict) and bbox.get(\"l\") is not None else 0.0\n",
        "\n",
        "            # Clause header detection\n",
        "            m = CLAUSE_RE.match(raw.strip())\n",
        "            if m:\n",
        "                cid = m.group(1)\n",
        "                rest = m.group(2).strip()\n",
        "\n",
        "                ensure_node(cid)\n",
        "                pid = parent_id(cid)\n",
        "                ensure_node(pid)\n",
        "                add_child(pid, cid)\n",
        "\n",
        "                depth = len(cid.split(\".\"))\n",
        "                if depth <= 3:\n",
        "                    if not nodes[cid][\"title\"]:\n",
        "                        nodes[cid][\"title\"] = rest\n",
        "                else:\n",
        "                    add_text_block(cid, rest)\n",
        "\n",
        "                current_id = cid\n",
        "                continue\n",
        "\n",
        "            # Regular list and text processing\n",
        "            marker_field = obj.get(\"marker\") or \"\"\n",
        "            mk, content, indent_spaces = parse_list_marker(raw_text=raw, marker_field=marker_field, allow_inline_start=True)\n",
        "\n",
        "            if label == \"list_item\" or mk:\n",
        "                if mk and content:\n",
        "                    add_list_block(current_id, mk, content, indent=geo_indent if geo_indent else float(indent_spaces), page_no=page_no, bbox=bbox)\n",
        "                else:\n",
        "                    add_text_block(current_id, normalize_ws(raw))\n",
        "            else:\n",
        "                add_text_block(current_id, normalize_ws(raw))\n",
        "        elif kind == \"tables\":\n",
        "            raw_id = f\"table_{idx:04d}\"\n",
        "            merged_id = table_id_map.get(raw_id, raw_id)\n",
        "            cap = table_caption_map.get(merged_id, \"\")\n",
        "\n",
        "            # Avoid duplicates when multiple page-fragments map to the same merged table\n",
        "            if not (nodes[current_id][\"tables\"] and nodes[current_id][\"tables\"][-1].get(\"table_id\") == merged_id):\n",
        "                nodes[current_id][\"tables\"].append({\"table_id\": merged_id, \"caption\": cap})\n",
        "\n",
        "        elif kind == \"pictures\":\n",
        "            raw_id = f\"figure_{idx:04d}\"\n",
        "            fig_id = figure_id_map.get(raw_id, raw_id)\n",
        "            cap = figure_caption_map.get(fig_id, \"\") or extract_caption(doc, obj.get(\"captions\"))\n",
        "            if not nodes[current_id][\"figures\"] or nodes[current_id][\"figures\"][-1].get(\"figure_id\") != fig_id:\n",
        "                nodes[current_id][\"figures\"].append({\"figure_id\": fig_id, \"caption\": cap})\n",
        "\n",
        "    # Finalize blocks into text + nested lists\n",
        "    for nid in list(nodes.keys()):\n",
        "        blocks = nodes[nid].get(\"_blocks\", [])\n",
        "        text, lists = blocks_to_text_and_lists(blocks)\n",
        "        nodes[nid][\"text\"] = text\n",
        "        nodes[nid][\"lists\"] = lists\n",
        "        nodes[nid].pop(\"_blocks\", None)\n",
        "\n",
        "    return {\"root\": root_id, \"nodes\": nodes}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Retrieval helper (optional)\n",
        "# ----------------------------\n",
        "def collect_text_recursive(structured: dict, clause_id: str) -> str:\n",
        "    nodes = structured[\"nodes\"]\n",
        "    if clause_id not in nodes:\n",
        "        return \"\"\n",
        "    n = nodes[clause_id]\n",
        "    chunks = []\n",
        "\n",
        "    if clause_id != \"ROOT\" and n.get(\"title\"):\n",
        "        chunks.append(f\"{clause_id} {n['title']}\".strip())\n",
        "    if n.get(\"text\"):\n",
        "        chunks.append(n[\"text\"])\n",
        "\n",
        "    for lst in n.get(\"lists\", []):\n",
        "        rendered = format_list_items(lst.get(\"items\", []))\n",
        "        if rendered:\n",
        "            chunks.append(rendered)\n",
        "\n",
        "    for fig in n.get(\"figures\", []):\n",
        "        if fig.get(\"caption\"):\n",
        "            chunks.append(fig[\"caption\"])\n",
        "    for tbl in n.get(\"tables\", []):\n",
        "        if tbl.get(\"caption\"):\n",
        "            chunks.append(tbl[\"caption\"])\n",
        "    for eq in n.get(\"equations\", []):\n",
        "        if eq.get(\"latex\"):\n",
        "            chunks.append(f\"$$ {eq['latex']} $$\")\n",
        "\n",
        "    for child in n.get(\"children\", []):\n",
        "        child_txt = collect_text_recursive(structured, child)\n",
        "        if child_txt:\n",
        "            chunks.append(child_txt)\n",
        "\n",
        "    return \"\\n\".join(chunks).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    out_base = Path(OUTPUT_DIR)\n",
        "    out_base.mkdir(parents=True, exist_ok=True)\n",
        "    images_dir = out_base / IMAGES_DIR_NAME\n",
        "    eq_dir = out_base / EQUATIONS_DIR_NAME\n",
        "\n",
        "    doc = load_docling_json(INPUT_DOCLING_JSON)\n",
        "\n",
        "    # Learn repeating headers/footers once and use it everywhere via is_noise_text()\n",
        "    global NOISE_CANONICALS\n",
        "    NOISE_CANONICALS = detect_repeating_headers_footers(doc)\n",
        "\n",
        "    # Build (and merge) tables first so clauses can reference merged IDs + captions\n",
        "    tables_struct, table_id_map = build_tables_json(doc)\n",
        "    table_caption_map = {t[\"table_id\"]: t.get(\"caption\", \"\") for t in tables_struct}\n",
        "\n",
        "    figures_struct, figure_id_map = build_figures_json(doc)\n",
        "    figure_caption_map = {f[\"figure_id\"]: f.get(\"caption\", \"\") for f in figures_struct}\n",
        "\n",
        "    clauses_struct = build_clause_tree(\n",
        "        doc,\n",
        "        table_id_map=table_id_map,\n",
        "        table_caption_map=table_caption_map,\n",
        "        figure_id_map=figure_id_map,\n",
        "        figure_caption_map=figure_caption_map,\n",
        "    )\n",
        "\n",
        "    images_struct = extract_figures_from_pdf(figures_struct, INPUT_PDF, str(images_dir), dpi=DPI, image_format=IMAGE_FORMAT)\n",
        "    equations_struct = extract_equations_from_pdf(doc, INPUT_PDF, str(eq_dir), dpi=DPI, image_format=IMAGE_FORMAT)\n",
        "\n",
        "    clauses_path = out_base / \"structured_clauses.json\"\n",
        "    tables_path = out_base / \"structured_tables.json\"\n",
        "    images_path = out_base / \"structured_images.json\"\n",
        "    equations_path = out_base / \"structured_equations.json\"\n",
        "\n",
        "    clauses_path.write_text(json.dumps(clauses_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    tables_path.write_text(json.dumps(tables_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    images_path.write_text(json.dumps(images_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    equations_path.write_text(json.dumps(equations_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # quick stats\n",
        "    nodes = clauses_struct[\"nodes\"]\n",
        "    list_blocks = sum(len(n.get(\"lists\", [])) for n in nodes.values())\n",
        "    list_items = 0\n",
        "    def count_items(items):\n",
        "        nonlocal list_items\n",
        "        for it in items:\n",
        "            list_items += 1\n",
        "            count_items(it.get(\"children\", []))\n",
        "    for n in nodes.values():\n",
        "        for lst in n.get(\"lists\", []):\n",
        "            count_items(lst.get(\"items\", []))\n",
        "\n",
        "    print(\"Saved:\")\n",
        "    print(\" -\", clauses_path)\n",
        "    print(\" -\", tables_path)\n",
        "    print(\" -\", images_path)\n",
        "    print(\" -\", equations_path)\n",
        "    print(\"\\nCounts:\")\n",
        "    print(\" clauses:\", len(nodes))\n",
        "    print(\" tables:\", len(tables_struct))\n",
        "    print(\" figures:\", len(images_struct))\n",
        "    print(\" equations:\", len(equations_struct))\n",
        "    print(\" list blocks:\", list_blocks)\n",
        "    print(\" list items:\", list_items)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#%%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRQtuEAZcmzS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}