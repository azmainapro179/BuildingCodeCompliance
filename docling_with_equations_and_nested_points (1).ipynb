{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GpNIa_LLRT",
        "outputId": "a619ef18-ada2-47b4-d69f-283d0a8d16f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.68.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.3)\n",
            "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading docling_core-2.59.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.12.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2026.1.4)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.4.1)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (6.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.12.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.9.0+cpu)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.15.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.26.0)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.3)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.0+cpu)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: opencv_python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading docling-2.68.0-py3-none-any.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.9/282.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.59.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.3/223.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.5.0-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (635 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=aed878634284fbdb3e2332a42a8e1f8b243742eeba47e7d9af70a2757e59fb08\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, filetype, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-c, tree-sitter, python-docx, pypdfium2, pyclipper, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.21.1\n",
            "    Uninstalling typer-0.21.1:\n",
            "      Successfully uninstalled typer-0.21.1\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.68.0 docling-core-2.59.0 docling-ibm-models-3.10.3 docling-parse-4.7.3 faker-40.1.2 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.2 mpire-2.10.2 polyfactory-3.2.0 pyclipper-1.4.0 pylatexenc-2.10 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.5.0 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install docling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "\n",
        "source = \"/content/bnbc6.pdf\"\n",
        "\n",
        "# Enable formula enrichment so equations become TextItems with label FORMULA and carry LaTeX text\n",
        "pipeline_options = PdfPipelineOptions()\n",
        "pipeline_options.do_formula_enrichment = True\n",
        "\n",
        "converter = DocumentConverter(\n",
        "    format_options={\n",
        "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "    }\n",
        ")\n",
        "\n",
        "result = converter.convert(source)\n",
        "\n",
        "# Full Docling JSON export\n",
        "structured_json = result.document.export_to_dict()\n",
        "\n",
        "print(\"Converted. First text item:\", structured_json[\"texts\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnpkfsE8LR6a",
        "outputId": "198726fc-96d2-4668-be08-4d88903a6bfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2026-01-15 08:44:36,604 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:36,610 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:36,613 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,278 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,442 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,444 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,786 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,787 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:38,789 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,744 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,758 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,759 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,855 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,856 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:39,858 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:40,656 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:40,918 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-15 08:44:40,920 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted. First text item: 3232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "output_filename = \"bnbc6.json\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Success! Your structured data is saved in {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5qhHbR6LyYb",
        "outputId": "56f8f255-c0df-4845-cdaf-a94e5d7ba4da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Your structured data is saved in bnbc6.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWdqL4VtcgbB",
        "outputId": "b8af44ae-3453-4948-90af-b6b28c4d0ddd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# ----------------------------\n",
        "# Config (edit these paths)\n",
        "# ----------------------------\n",
        "INPUT_DOCLING_JSON = \"/content/bnbc6.json\"\n",
        "INPUT_PDF = \"/content/bnbc6.pdf\"\n",
        "\n",
        "OUTPUT_DIR = \"/content/structured_out\"\n",
        "IMAGES_DIR_NAME = \"images\"\n",
        "EQUATIONS_DIR_NAME = \"equations\"\n",
        "DPI = 200\n",
        "IMAGE_FORMAT = \"jpg\"\n",
        "\n",
        "# If Docling formula enrichment produces spaced-out LaTeX (each char separated by spaces),\n",
        "# enable this heuristic de-spacing fix.\n",
        "FIX_SPACED_LATEX = True\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers: Docling JSON access\n",
        "# ----------------------------\n",
        "CLAUSE_RE = re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)\\s+(.*\\S)\\s*$\")\n",
        "\n",
        "\n",
        "def normalize_ws(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
        "\n",
        "\n",
        "def load_docling_json(path: str) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def resolve_ref(doc: dict, ref: str):\n",
        "    \"\"\"\n",
        "    ref examples:\n",
        "      '#/texts/93'\n",
        "      '#/tables/0'\n",
        "      '#/pictures/2'\n",
        "      '#/groups/5'\n",
        "    \"\"\"\n",
        "    m = re.match(r\"^#/(texts|tables|pictures|groups)/(\\d+)$\", ref)\n",
        "    if not m:\n",
        "        return None, None, None\n",
        "    kind, idx = m.group(1), int(m.group(2))\n",
        "    return kind, idx, doc[kind][idx]\n",
        "\n",
        "\n",
        "def extract_caption(doc: dict, captions) -> str:\n",
        "    \"\"\"\n",
        "    captions is typically a list like [{'$ref': '#/texts/93'}]\n",
        "    \"\"\"\n",
        "    if not captions:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for c in captions:\n",
        "        r = c.get(\"$ref\")\n",
        "        if not r:\n",
        "            continue\n",
        "        kind, _, obj = resolve_ref(doc, r)\n",
        "        if kind == \"texts\":\n",
        "            t = normalize_ws(obj.get(\"text\") or \"\")\n",
        "            if t:\n",
        "                parts.append(t)\n",
        "    return \" \".join(parts).strip()\n",
        "\n",
        "\n",
        "def walk_body_in_reading_order(doc: dict):\n",
        "    \"\"\"\n",
        "    Body children contains a mixture of texts/tables/pictures and also '#/groups/*'.\n",
        "    We DFS into groups so we don't miss content.\n",
        "    \"\"\"\n",
        "    body = doc.get(\"body\") or {}\n",
        "    stack = []\n",
        "\n",
        "    def push_children(children):\n",
        "        for child in reversed(children or []):\n",
        "            stack.append(child)\n",
        "\n",
        "    push_children(body.get(\"children\"))\n",
        "\n",
        "    while stack:\n",
        "        item = stack.pop()\n",
        "        if not isinstance(item, dict) or \"$ref\" not in item:\n",
        "            continue\n",
        "\n",
        "        ref = item[\"$ref\"]\n",
        "        kind, _, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"groups\":\n",
        "            push_children(obj.get(\"children\"))\n",
        "            continue\n",
        "\n",
        "        yield ref\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Generic nested point/list parsing (handles (a), (1), (i), bullets, etc.)\n",
        "# ----------------------------\n",
        "PAREN_MARKER_RE = re.compile(r\"^\\s*\\(\\s*([A-Za-z0-9]+|[ivxlcdmIVXLCDM]+)\\s*\\)\\s+\")\n",
        "SIMPLE_MARKER_RE = re.compile(r\"^\\s*([A-Za-z]|\\d+|[ivxlcdmIVXLCDM]+)([\\.|\\)])\\s+\")\n",
        "BULLET_MARKER_RE = re.compile(r\"^\\s*([•\\-*–—])\\s+\")\n",
        "\n",
        "\n",
        "def leading_indent(s: str) -> int:\n",
        "    # Count leading spaces (tabs are rare in PDFs; treat them as 4 spaces if present)\n",
        "    if not s:\n",
        "        return 0\n",
        "    n = 0\n",
        "    for ch in s:\n",
        "        if ch == \" \":\n",
        "            n += 1\n",
        "        elif ch == \"\\t\":\n",
        "            n += 4\n",
        "        else:\n",
        "            break\n",
        "    return n\n",
        "\n",
        "\n",
        "def parse_list_marker(raw_text: str, marker_field: str = \"\", allow_inline: bool = True):\n",
        "    \"\"\"\n",
        "    Returns (marker, content, indent) if this looks like a point item; otherwise (None, None, None).\n",
        "\n",
        "    Priority:\n",
        "      1) Docling-provided marker_field (e.g., '1.' or '*')\n",
        "      2) '(a) ...', '(1) ...', '(i) ...' in the text\n",
        "      3) 'a) ...', '1. ...', 'i) ...' in the text\n",
        "      4) bullet char in the text\n",
        "\n",
        "    Notes:\n",
        "      - We treat clause headers like '5.10.2.1 ...' separately before calling this.\n",
        "      - allow_inline: if True, we attempt inline markers when no marker_field is present.\n",
        "    \"\"\"\n",
        "    if raw_text is None:\n",
        "        return None, None, None\n",
        "\n",
        "    indent = leading_indent(raw_text)\n",
        "    s = raw_text.lstrip(\"\\t \").rstrip()\n",
        "\n",
        "    # 1) marker field from Docling (often for enumerations or bullets)\n",
        "    mf = (marker_field or \"\").strip()\n",
        "    if mf:\n",
        "        # Docling's `text` usually excludes the marker when marker_field is set\n",
        "        content = normalize_ws(s)\n",
        "        return mf, content, indent\n",
        "\n",
        "    if not allow_inline or not s:\n",
        "        return None, None, None\n",
        "\n",
        "    # 2) (a) / (1) / (i)\n",
        "    m = PAREN_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"({m.group(1)})\"\n",
        "        content = normalize_ws(s[m.end():])\n",
        "        return mk, content, indent\n",
        "\n",
        "    # 3) a) / 1. / i)\n",
        "    m = SIMPLE_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = f\"{m.group(1)}{m.group(2)}\"\n",
        "        content = normalize_ws(s[m.end():])\n",
        "        return mk, content, indent\n",
        "\n",
        "    # 4) bullets in text\n",
        "    m = BULLET_MARKER_RE.match(s)\n",
        "    if m:\n",
        "        mk = m.group(1)\n",
        "        content = normalize_ws(s[m.end():])\n",
        "        return mk, content, indent\n",
        "\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "def _is_roman(s: str) -> bool:\n",
        "    if not s:\n",
        "        return False\n",
        "    s = s.strip().lower()\n",
        "    return bool(re.fullmatch(r\"[ivxlcdm]+\", s))\n",
        "\n",
        "\n",
        "def marker_kind(marker: str) -> str:\n",
        "    \"\"\"\n",
        "    Classify marker into kinds:\n",
        "      \"(1)\", \"1.\", \"2)\" -> \"num\"\n",
        "      \"(a)\", \"a)\"       -> \"alpha\"\n",
        "      \"(i)\", \"iv.\"      -> \"roman\"\n",
        "      \"•\", \"-\", \"*\"     -> \"bullet\"\n",
        "    \"\"\"\n",
        "    if not marker:\n",
        "        return \"other\"\n",
        "    m = marker.strip()\n",
        "\n",
        "    # bullets\n",
        "    if BULLET_MARKER_RE.match(m) or m in {\"•\", \"-\", \"*\", \"–\", \"—\"}:\n",
        "        return \"bullet\"\n",
        "\n",
        "    # strip wrapping punctuation\n",
        "    core = re.sub(r\"^[\\(\\[]\\s*\", \"\", m)\n",
        "    core = re.sub(r\"\\s*[\\)\\]]$\", \"\", core)\n",
        "    core = re.sub(r\"[.)]$\", \"\", core).strip()\n",
        "\n",
        "    if core.isdigit():\n",
        "        return \"num\"\n",
        "    if len(core) == 1 and core.isalpha():\n",
        "        return \"alpha\"\n",
        "    if _is_roman(core):\n",
        "        return \"roman\"\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "KIND_ORDER = {\"num\": 0, \"alpha\": 1, \"roman\": 2, \"bullet\": 3, \"other\": 3}\n",
        "\n",
        "\n",
        "def _nesting_trigger_text(s: str) -> bool:\n",
        "    \"\"\"\n",
        "    Strong cue that a sublist follows. Your example uses ':'.\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return False\n",
        "    t = s.strip().lower()\n",
        "    if t.endswith(\":\"):\n",
        "        return True\n",
        "    cues = [\n",
        "        \"as follows:\",\n",
        "        \"the following:\",\n",
        "        \"the following criteria:\",\n",
        "        \"meets all of the following criteria:\",\n",
        "        \"all of the following:\",\n",
        "        \"based on:\",\n",
        "        \"shall include:\",\n",
        "        \"shall be based on:\",\n",
        "    ]\n",
        "    return any(t.endswith(c) for c in cues)\n",
        "\n",
        "\n",
        "def _will_return_to_kind(flat_items, start_idx: int, target_kind: str) -> bool:\n",
        "    \"\"\"\n",
        "    Lookahead heuristic:\n",
        "      (6) then (a)(b)(c) then (7)  => (a)(b)(c) belong under (6)\n",
        "    \"\"\"\n",
        "    for j in range(start_idx + 1, len(flat_items)):\n",
        "        k = marker_kind(flat_items[j].get(\"marker\", \"\"))\n",
        "        if k == target_kind:\n",
        "            return True\n",
        "        # if we see something shallower than target_kind, stop\n",
        "        if KIND_ORDER.get(k, 3) < KIND_ORDER.get(target_kind, 3):\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "\n",
        "def nest_list_items_smart(flat_items):\n",
        "    \"\"\"\n",
        "    Build a nested list tree using:\n",
        "      1) indentation if informative\n",
        "      2) marker-kind hierarchy + cues (':' / 'following criteria:' / lookahead return)\n",
        "    \"\"\"\n",
        "    root = []\n",
        "    stack = []  # entries: {\"indent\": int, \"kind\": str, \"order\": int, \"node\": dict}\n",
        "\n",
        "    for i, it in enumerate(flat_items):\n",
        "        node = {\"marker\": it.get(\"marker\", \"\"), \"text\": it.get(\"text\", \"\"), \"children\": []}\n",
        "        ind = int(it.get(\"indent\") or 0)\n",
        "\n",
        "        k = marker_kind(node[\"marker\"])\n",
        "        k_order = KIND_ORDER.get(k, 3)\n",
        "\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        prev = stack[-1]\n",
        "\n",
        "        # 1) Trust indentation if it actually changes\n",
        "        indent_diff = ind - prev[\"indent\"]\n",
        "        indent_is_informative = abs(indent_diff) >= 2  # tune if needed\n",
        "\n",
        "        if indent_is_informative:\n",
        "            while stack and ind <= stack[-1][\"indent\"]:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # 2) Marker-based inference\n",
        "        prev_kind = prev[\"kind\"]\n",
        "        prev_order = prev[\"order\"]\n",
        "\n",
        "        prev_introduces_sublist = _nesting_trigger_text(prev[\"node\"].get(\"text\", \"\"))\n",
        "        lookahead_sublist = _will_return_to_kind(flat_items, i, prev_kind)\n",
        "\n",
        "        # Deeper kind becomes child only if we have a cue (prevents over-nesting)\n",
        "        if (k_order > prev_order) and (prev_introduces_sublist or lookahead_sublist):\n",
        "            prev[\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # Same kind => sibling\n",
        "        if k == prev_kind:\n",
        "            stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # Shallower (or different but not deeper) => pop to a compatible parent\n",
        "        if k_order <= prev_order:\n",
        "            while stack and KIND_ORDER.get(stack[-1][\"kind\"], 3) >= k_order:\n",
        "                stack.pop()\n",
        "            if not stack:\n",
        "                root.append(node)\n",
        "            else:\n",
        "                stack[-1][\"node\"][\"children\"].append(node)\n",
        "            stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "            continue\n",
        "\n",
        "        # Default fallback: sibling of previous\n",
        "        stack.pop()\n",
        "        if not stack:\n",
        "            root.append(node)\n",
        "        else:\n",
        "            stack[-1][\"node\"][\"children\"].append(node)\n",
        "        stack.append({\"indent\": ind, \"kind\": k, \"order\": k_order, \"node\": node})\n",
        "\n",
        "    return root\n",
        "\n",
        "def blocks_to_text_and_lists(blocks):\n",
        "    \"\"\"\n",
        "    blocks: list of either\n",
        "      {\"kind\":\"text\",\"text\":...}\n",
        "      {\"kind\":\"list_item\",\"marker\":..., \"text\":..., \"indent\":...}\n",
        "\n",
        "    Returns:\n",
        "      text: merged paragraphs (non-list blocks)\n",
        "      lists: list of {\"items\":[nested...]} in reading order\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "    lists = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(blocks):\n",
        "        b = blocks[i]\n",
        "        if b[\"kind\"] == \"text\":\n",
        "            t = b.get(\"text\", \"\")\n",
        "            if t:\n",
        "                text_parts.append(t.strip())\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # collect a contiguous run of list items into one list block\n",
        "        j = i\n",
        "        run = []\n",
        "        while j < len(blocks) and blocks[j][\"kind\"] == \"list_item\":\n",
        "            run.append(blocks[j])\n",
        "            j += 1\n",
        "\n",
        "        nested = nest_list_items_smart(run)\n",
        "        lists.append({\"items\": nested})\n",
        "        i = j\n",
        "\n",
        "    text = \"\\n\".join([p for p in text_parts if p]).strip()\n",
        "    return text, lists\n",
        "\n",
        "\n",
        "def format_list_items(items, level=0):\n",
        "    \"\"\"\n",
        "    Render nested list items into plain text (for retrieval display).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    indent = \"  \" * level\n",
        "    for it in items or []:\n",
        "        mk = it.get(\"marker\", \"\")\n",
        "        tx = it.get(\"text\", \"\")\n",
        "        line = f\"{indent}{mk} {tx}\".strip()\n",
        "        if line:\n",
        "            out.append(line)\n",
        "        if it.get(\"children\"):\n",
        "            out.append(format_list_items(it[\"children\"], level + 1))\n",
        "    return \"\\n\".join([x for x in out if x]).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Tables (FIXED)\n",
        "# ----------------------------\n",
        "def table_rows_robust(table_obj: dict):\n",
        "    \"\"\"\n",
        "    Robustly converts Docling tables into rows[][].\n",
        "\n",
        "    Handles:\n",
        "      A) data[\"grid\"][r][c] is a dict with \"text\"\n",
        "      B) data[\"grid\"][r][c] is an int index into data[\"table_cells\"]\n",
        "      C) missing/empty grid -> reconstruct from table_cells span metadata\n",
        "    \"\"\"\n",
        "    data = table_obj.get(\"data\") or {}\n",
        "    grid = data.get(\"grid\") or []\n",
        "    table_cells = data.get(\"table_cells\") or []\n",
        "\n",
        "    num_rows = int(data.get(\"num_rows\") or (len(grid) if grid else 0))\n",
        "    num_cols = int(data.get(\"num_cols\") or (len(grid[0]) if grid and grid[0] else 0))\n",
        "\n",
        "    # Case A: grid holds dict cells directly\n",
        "    if grid and grid[0] and isinstance(grid[0][0], dict):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for cell in r:\n",
        "                if isinstance(cell, dict):\n",
        "                    row.append(normalize_ws(cell.get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    # Case B: grid holds ints referencing table_cells\n",
        "    if grid and grid[0] and isinstance(grid[0][0], int):\n",
        "        rows = []\n",
        "        for r in grid:\n",
        "            row = []\n",
        "            for idx in r:\n",
        "                if isinstance(idx, int) and 0 <= idx < len(table_cells):\n",
        "                    row.append(normalize_ws(table_cells[idx].get(\"text\", \"\")))\n",
        "                else:\n",
        "                    row.append(\"\")\n",
        "            rows.append(row)\n",
        "        return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": rows}\n",
        "\n",
        "    # Case C: reconstruct from table_cells span metadata\n",
        "    mat = [[\"\" for _ in range(num_cols)] for _ in range(num_rows)]\n",
        "    for cell in table_cells:\n",
        "        txt = normalize_ws(cell.get(\"text\", \"\"))\n",
        "        r0 = cell.get(\"start_row_offset_idx\", 0)\n",
        "        r1 = cell.get(\"end_row_offset_idx\", r0 + 1)\n",
        "        c0 = cell.get(\"start_col_offset_idx\", 0)\n",
        "        c1 = cell.get(\"end_col_offset_idx\", c0 + 1)\n",
        "        for rr in range(r0, r1):\n",
        "            for cc in range(c0, c1):\n",
        "                if 0 <= rr < num_rows and 0 <= cc < num_cols:\n",
        "                    mat[rr][cc] = txt\n",
        "\n",
        "    return {\"num_rows\": num_rows, \"num_cols\": num_cols, \"rows\": mat}\n",
        "\n",
        "\n",
        "def build_tables_json(doc: dict) -> list:\n",
        "    out = []\n",
        "    for i, tbl in enumerate(doc.get(\"tables\", [])):\n",
        "        provs = tbl.get(\"prov\") or []\n",
        "        page_no = provs[0].get(\"page_no\") if provs else None\n",
        "        bbox = provs[0].get(\"bbox\") if provs else None\n",
        "        caption = extract_caption(doc, tbl.get(\"captions\"))\n",
        "\n",
        "        t = table_rows_robust(tbl)\n",
        "\n",
        "        out.append(\n",
        "            {\n",
        "                \"table_id\": f\"table_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"num_rows\": t[\"num_rows\"],\n",
        "                \"num_cols\": t[\"num_cols\"],\n",
        "                \"rows\": t[\"rows\"],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Images & geometry helpers\n",
        "# ----------------------------\n",
        "def bbox_to_fitz_rect(bbox: dict, page_height: float) -> fitz.Rect | None:\n",
        "    \"\"\"\n",
        "    Docling bbox has coord_origin, often 'BOTTOMLEFT'.\n",
        "    PyMuPDF uses TOPLEFT origin.\n",
        "    \"\"\"\n",
        "    l = bbox.get(\"l\")\n",
        "    t = bbox.get(\"t\")\n",
        "    r = bbox.get(\"r\")\n",
        "    b = bbox.get(\"b\")\n",
        "    if None in (l, t, r, b):\n",
        "        return None\n",
        "\n",
        "    origin = (bbox.get(\"coord_origin\") or \"TOPLEFT\").upper()\n",
        "\n",
        "    if origin == \"BOTTOMLEFT\":\n",
        "        # Convert y coords from bottom-origin to top-origin\n",
        "        y0 = page_height - t\n",
        "        y1 = page_height - b\n",
        "    else:\n",
        "        y0 = t\n",
        "        y1 = b\n",
        "\n",
        "    x0, x1 = sorted([l, r])\n",
        "    y0, y1 = sorted([y0, y1])\n",
        "    return fitz.Rect(x0, y0, x1, y1)\n",
        "\n",
        "\n",
        "def extract_figures_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    images_meta = []\n",
        "\n",
        "    for i, pic in enumerate(doc.get(\"pictures\", [])):\n",
        "        provs = pic.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]  # usually only one\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        page_rect = page.rect\n",
        "\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page_rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page_rect  # clamp to page\n",
        "        caption = extract_caption(doc, pic.get(\"captions\"))\n",
        "\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        fname = f\"figure_{i:04d}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        images_meta.append(\n",
        "            {\n",
        "                \"figure_id\": f\"figure_{i:04d}\",\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"caption\": caption,\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return images_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Equations (FORMULA items)\n",
        "# ----------------------------\n",
        "def is_formula_text_item(text_obj: dict) -> bool:\n",
        "    return (text_obj.get(\"label\") or \"\").strip().upper() == \"FORMULA\"\n",
        "\n",
        "\n",
        "def maybe_despace_latex(latex: str) -> str:\n",
        "    \"\"\"\n",
        "    Heuristic fix for cases where each character is separated by spaces.\n",
        "    If most tokens are single characters, we join without spaces.\n",
        "    \"\"\"\n",
        "    s = latex.strip()\n",
        "    toks = s.split()\n",
        "    if len(toks) < 8:\n",
        "        return s\n",
        "    single = sum(1 for t in toks if len(t) == 1)\n",
        "    if single / max(1, len(toks)) >= 0.6:\n",
        "        return \"\".join(toks)\n",
        "    return s\n",
        "\n",
        "\n",
        "def extract_formula_latex(text_obj: dict) -> str:\n",
        "    # Some versions may store latex in a dedicated field; otherwise it's in \"text\"\n",
        "    raw = text_obj.get(\"latex\") or text_obj.get(\"text\") or \"\"\n",
        "    raw = raw.strip()\n",
        "    raw = normalize_ws(raw)\n",
        "    if FIX_SPACED_LATEX:\n",
        "        raw = maybe_despace_latex(raw)\n",
        "    return raw\n",
        "\n",
        "\n",
        "def extract_equations_from_pdf(doc: dict, pdf_path: str, out_dir: str, dpi: int, image_format: str):\n",
        "    \"\"\"\n",
        "    Crops each FORMULA text item from the PDF using its provenance bbox and stores it as an image.\n",
        "    Also returns a list of equation metadata including LaTeX.\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pdf = fitz.open(pdf_path)\n",
        "    equations_meta = []\n",
        "\n",
        "    texts = doc.get(\"texts\", []) or []\n",
        "    for idx, t in enumerate(texts):\n",
        "        if not is_formula_text_item(t):\n",
        "            continue\n",
        "\n",
        "        provs = t.get(\"prov\") or []\n",
        "        if not provs:\n",
        "            continue\n",
        "\n",
        "        prov = provs[0]\n",
        "        page_no = prov.get(\"page_no\")\n",
        "        bbox = prov.get(\"bbox\")\n",
        "        if not page_no or not bbox:\n",
        "            continue\n",
        "\n",
        "        page_index = page_no - 1\n",
        "        if page_index < 0 or page_index >= pdf.page_count:\n",
        "            continue\n",
        "\n",
        "        page = pdf.load_page(page_index)\n",
        "        page_rect = page.rect\n",
        "\n",
        "        clip = bbox_to_fitz_rect(bbox, page_height=page_rect.height)\n",
        "        if clip is None:\n",
        "            continue\n",
        "\n",
        "        clip = clip & page_rect\n",
        "        zoom = dpi / 72.0\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom), clip=clip, alpha=False)\n",
        "\n",
        "        eq_id = f\"eq_{idx:05d}\"\n",
        "        fname = f\"{eq_id}_p{page_no}.{image_format}\"\n",
        "        fpath = out_dir / fname\n",
        "        pix.save(str(fpath))\n",
        "\n",
        "        equations_meta.append(\n",
        "            {\n",
        "                \"equation_id\": eq_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"bbox\": bbox,\n",
        "                \"latex\": extract_formula_latex(t),\n",
        "                \"file\": str(fpath),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    pdf.close()\n",
        "    return equations_meta\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Clause tree (now includes equations + generic nested points)\n",
        "# ----------------------------\n",
        "def build_clause_tree(doc: dict):\n",
        "    \"\"\"\n",
        "    Builds nodes keyed by clause id (e.g., '5.5', '5.5.1', '5.5.1.1').\n",
        "\n",
        "    - Figures: stored as captions only (plus id).\n",
        "    - Tables: referenced by id + caption.\n",
        "    - Equations: referenced by id + LaTeX (extracted via do_formula_enrichment).\n",
        "    - Nested points/lists: parsed from Docling list items + inline markers into `lists` with children.\n",
        "      Markers can be (a), (1), (i), bullets, '1.', 'a)', etc.\n",
        "    \"\"\"\n",
        "    nodes = {}\n",
        "    root_id = \"ROOT\"\n",
        "    nodes[root_id] = {\n",
        "        \"id\": root_id,\n",
        "        \"title\": \"\",\n",
        "        \"children\": [],\n",
        "        \"tables\": [],\n",
        "        \"figures\": [],\n",
        "        \"equations\": [],\n",
        "        # final outputs\n",
        "        \"text\": \"\",\n",
        "        \"lists\": [],\n",
        "        # internal parsing buffer\n",
        "        \"_blocks\": [],\n",
        "    }\n",
        "    current_id = root_id\n",
        "\n",
        "    def ensure_node(cid: str):\n",
        "        if cid not in nodes:\n",
        "            nodes[cid] = {\n",
        "                \"id\": cid,\n",
        "                \"title\": \"\",\n",
        "                \"children\": [],\n",
        "                \"tables\": [],\n",
        "                \"figures\": [],\n",
        "                \"equations\": [],\n",
        "                \"text\": \"\",\n",
        "                \"lists\": [],\n",
        "                \"_blocks\": [],\n",
        "            }\n",
        "\n",
        "    def parent_id(cid: str) -> str:\n",
        "        parts = cid.split(\".\")\n",
        "        return root_id if len(parts) <= 1 else \".\".join(parts[:-1])\n",
        "\n",
        "    def add_child(pid: str, cid: str):\n",
        "        if cid not in nodes[pid][\"children\"]:\n",
        "            nodes[pid][\"children\"].append(cid)\n",
        "\n",
        "    def add_text_block(cid: str, txt: str):\n",
        "        txt = txt.rstrip()\n",
        "        if not txt:\n",
        "            return\n",
        "        nodes[cid][\"_blocks\"].append({\"kind\": \"text\", \"text\": txt})\n",
        "\n",
        "    def add_list_block(cid: str, marker: str, txt: str, indent: int):\n",
        "        nodes[cid][\"_blocks\"].append({\"kind\": \"list_item\", \"marker\": marker, \"text\": txt, \"indent\": indent})\n",
        "\n",
        "    for ref in walk_body_in_reading_order(doc):\n",
        "        kind, idx, obj = resolve_ref(doc, ref)\n",
        "\n",
        "        if kind == \"texts\":\n",
        "            label = (obj.get(\"label\") or \"\").strip().lower()\n",
        "            raw = obj.get(\"text\") or \"\"\n",
        "            if not raw.strip():\n",
        "                continue\n",
        "\n",
        "            # Clause header detection should happen BEFORE list parsing\n",
        "            m = CLAUSE_RE.match(raw.strip())\n",
        "            if m:\n",
        "                cid = m.group(1)\n",
        "                rest = m.group(2).strip()\n",
        "\n",
        "                ensure_node(cid)\n",
        "                pid = parent_id(cid)\n",
        "                ensure_node(pid)\n",
        "                add_child(pid, cid)\n",
        "\n",
        "                depth = len(cid.split(\".\"))\n",
        "\n",
        "                if depth <= 3:\n",
        "                    if not nodes[cid][\"title\"]:\n",
        "                        nodes[cid][\"title\"] = rest\n",
        "                    # headings don't go into body text blocks\n",
        "                else:\n",
        "                    add_text_block(cid, rest)\n",
        "\n",
        "                current_id = cid\n",
        "                continue\n",
        "\n",
        "            # Handle equations (FORMULA items)\n",
        "            if is_formula_text_item(obj):\n",
        "                eq_id = f\"eq_{idx:05d}\"\n",
        "                latex = extract_formula_latex(obj)\n",
        "                nodes[current_id][\"equations\"].append({\"equation_id\": eq_id, \"latex\": latex})\n",
        "                if latex:\n",
        "                    add_text_block(current_id, f\"$$ {latex} $$\")\n",
        "                else:\n",
        "                    add_text_block(current_id, f\"[EQ {eq_id}]\")\n",
        "                continue\n",
        "\n",
        "            # Handle list items (Docling label=list_item OR inline markers)\n",
        "            marker_field = obj.get(\"marker\") or \"\"\n",
        "            mk, content, ind = parse_list_marker(raw_text=raw, marker_field=marker_field, allow_inline=True)\n",
        "\n",
        "            # If Docling says it's a list_item OR we successfully detect a marker, treat as point\n",
        "            if label == \"list_item\" or mk:\n",
        "                # Some list_items are not really enumerated; only store as list if marker detected\n",
        "                if mk and content:\n",
        "                    add_list_block(current_id, mk, content, ind)\n",
        "                else:\n",
        "                    # No marker detected: treat as plain text\n",
        "                    add_text_block(current_id, normalize_ws(raw))\n",
        "            else:\n",
        "                # Regular paragraph/text\n",
        "                add_text_block(current_id, normalize_ws(raw))\n",
        "\n",
        "        elif kind == \"tables\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"tables\"].append({\"table_id\": f\"table_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "        elif kind == \"pictures\":\n",
        "            caption = extract_caption(doc, obj.get(\"captions\"))\n",
        "            nodes[current_id][\"figures\"].append({\"figure_id\": f\"figure_{idx:04d}\", \"caption\": caption})\n",
        "\n",
        "    # Finalize: turn blocks into `text` + `lists`\n",
        "    for nid in list(nodes.keys()):\n",
        "        blocks = nodes[nid].get(\"_blocks\", [])\n",
        "        text, lists = blocks_to_text_and_lists(blocks)\n",
        "        nodes[nid][\"text\"] = text\n",
        "        nodes[nid][\"lists\"] = lists\n",
        "        nodes[nid].pop(\"_blocks\", None)\n",
        "\n",
        "    return {\"root\": root_id, \"nodes\": nodes}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Optional: retrieval helper\n",
        "# ----------------------------\n",
        "def collect_text_recursive(structured: dict, clause_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns clause text + list items + descendants' text.\n",
        "    \"\"\"\n",
        "    nodes = structured[\"nodes\"]\n",
        "    if clause_id not in nodes:\n",
        "        return \"\"\n",
        "\n",
        "    n = nodes[clause_id]\n",
        "    chunks = []\n",
        "\n",
        "    if clause_id != \"ROOT\" and n.get(\"title\"):\n",
        "        chunks.append(f\"{clause_id} {n['title']}\".strip())\n",
        "    if n.get(\"text\"):\n",
        "        chunks.append(n[\"text\"])\n",
        "\n",
        "    # include nested points\n",
        "    for lst in n.get(\"lists\", []):\n",
        "        rendered = format_list_items(lst.get(\"items\", []))\n",
        "        if rendered:\n",
        "            chunks.append(rendered)\n",
        "\n",
        "    # include captions / latex so outputs stay informative without embedding images/tables\n",
        "    for fig in n.get(\"figures\", []):\n",
        "        if fig.get(\"caption\"):\n",
        "            chunks.append(fig[\"caption\"])\n",
        "    for tbl in n.get(\"tables\", []):\n",
        "        if tbl.get(\"caption\"):\n",
        "            chunks.append(tbl[\"caption\"])\n",
        "    for eq in n.get(\"equations\", []):\n",
        "        if eq.get(\"latex\"):\n",
        "            chunks.append(f\"$$ {eq['latex']} $$\")\n",
        "\n",
        "    for child in n.get(\"children\", []):\n",
        "        child_txt = collect_text_recursive(structured, child)\n",
        "        if child_txt:\n",
        "            chunks.append(child_txt)\n",
        "\n",
        "    return \"\\n\".join(chunks).strip()\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    out_base = Path(OUTPUT_DIR)\n",
        "    out_base.mkdir(parents=True, exist_ok=True)\n",
        "    images_dir = out_base / IMAGES_DIR_NAME\n",
        "    eq_dir = out_base / EQUATIONS_DIR_NAME\n",
        "\n",
        "    doc = load_docling_json(INPUT_DOCLING_JSON)\n",
        "\n",
        "    # 1) Clause tree (with figure captions + equation latex refs + nested points)\n",
        "    clauses_struct = build_clause_tree(doc)\n",
        "\n",
        "    # 2) Tables as rows\n",
        "    tables_struct = build_tables_json(doc)\n",
        "\n",
        "    # 3) Figures as JPG + metadata\n",
        "    images_struct = extract_figures_from_pdf(\n",
        "        doc=doc,\n",
        "        pdf_path=INPUT_PDF,\n",
        "        out_dir=str(images_dir),\n",
        "        dpi=DPI,\n",
        "        image_format=IMAGE_FORMAT,\n",
        "    )\n",
        "\n",
        "    # 4) Equations as cropped JPG + LaTeX metadata (requires do_formula_enrichment=True during conversion)\n",
        "    equations_struct = extract_equations_from_pdf(\n",
        "        doc=doc,\n",
        "        pdf_path=INPUT_PDF,\n",
        "        out_dir=str(eq_dir),\n",
        "        dpi=DPI,\n",
        "        image_format=IMAGE_FORMAT,\n",
        "    )\n",
        "\n",
        "    # Save outputs\n",
        "    clauses_path = out_base / \"structured_clauses.json\"\n",
        "    tables_path = out_base / \"structured_tables.json\"\n",
        "    images_path = out_base / \"structured_images.json\"\n",
        "    equations_path = out_base / \"structured_equations.json\"\n",
        "\n",
        "    clauses_path.write_text(json.dumps(clauses_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    tables_path.write_text(json.dumps(tables_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    images_path.write_text(json.dumps(images_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    equations_path.write_text(json.dumps(equations_struct, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    # Quick sanity check\n",
        "    nodes = clauses_struct[\"nodes\"]\n",
        "    list_items_count = sum(len(n.get(\"lists\", [])) for n in nodes.values())\n",
        "    print(\"Saved:\")\n",
        "    print(\" -\", clauses_path)\n",
        "    print(\" -\", tables_path)\n",
        "    print(\" -\", images_path)\n",
        "    print(\" -\", equations_path)\n",
        "    print(\"Images saved under:\", images_dir)\n",
        "    print(\"Equations saved under:\", eq_dir)\n",
        "    print(\"\\nCounts:\")\n",
        "    print(\" clauses:\", len(nodes))\n",
        "    print(\" tables:\", len(tables_struct))\n",
        "    print(\" figures:\", len(images_struct))\n",
        "    print(\" equations:\", len(equations_struct))\n",
        "    print(\" list blocks:\", list_items_count)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewSaeLuiU9VM",
        "outputId": "d92efcb2-8871-47d1-96a8-faffcba409f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            " - /content/structured_out/structured_clauses.json\n",
            " - /content/structured_out/structured_tables.json\n",
            " - /content/structured_out/structured_images.json\n",
            " - /content/structured_out/structured_equations.json\n",
            "Images saved under: /content/structured_out/images\n",
            "Equations saved under: /content/structured_out/equations\n",
            "\n",
            "Counts:\n",
            " clauses: 15\n",
            " tables: 5\n",
            " figures: 0\n",
            " equations: 13\n",
            " list blocks: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRQtuEAZcmzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}